{
  "Lec24_slides": {
    "url": "https://lass.cs.umass.edu/~shenoy/courses/677content/slides/spring25/Lec24.pdf",
    "parsed_text": "Distributed Middleware\n• Distributed objects\n• EJBs\n• DCOM\n• CORBA\n• Jini\n• Distributed Data Processing\n–Hadoop\n–Spark\nCompsci 677: Distributed and OS Lec. 24 1\nDistributed Objects\n• Figure 10-1. Common organization of a remote\nobject with client-side proxy.\nCompsci 677: Distributed and OS Lec. 24 2\n\nExample: Enterprise Java Beans\n• Figure 10-2. General\narchitecture of an EJB\nserver.\nCompsci 677: Distributed and OS Lec. 24 3\nParts of an EJB\n• Home interface:\n– Object creation, deletion\n– Location of persistent objects (entity beans)\n– Object identifier is class-managed\n• Remote interface\n– “business logic”\n– i.e. the object itself\n• Terminology differences\n– Client/server -> web applications\nCompsci 677: Distributed and OS Lec. 24 4\n\nFour Types of EJBs\n• Stateless session beans\n• Stateful session beans\n• Entity beans - persist state on disk\n• Message-driven beans\nCompsci 677: Distributed and OS Lec. 24 5\nCORBA Overview\n• Object request broker (ORB)\n– Core of the middleware platform\n– Handles communication between objects and clients\n– Handles distribution and heterogeneity issues\n– May be implemented as libraries\n• Facilities: composition of CORBA services\nCompsci 677: Distributed and OS Lec. 24 6\n\nCorba Services\nService Description\nCollection Facilities for grouping objects into lists, queue, sets, etc.\nQuery Facilities for querying collections of objects in a declarative manner\nConcurrency Facilities to allow concurrent access to shared objects\nTransaction Flat and nested transactions on method calls over multiple objects\nEvent Facilities for asynchronous communication through events\nNotification Advanced facilities for event-based asynchronous communication\nExternalization Facilities for marshaling and unmarshaling of objects\nLife cycle Facilities for creation, deletion, copying, and moving of objects\nLicensing Facilities for attaching a license to an object\nNaming Facilities for systemwide name of objects\nProperty Facilities for associating (attribute, value) pairs with objects\nTrading Facilities to publish and find the services on object has to offer\nPersistence Facilities for persistently storing objects\nRelationship Facilities for expressing relationships between objects\nSecurity Mechanisms for secure channels, authorization, and auditing\nTime Provides the current time within specified error margins\nCompsci 677: Distributed and OS Lec. 24 7\nObject Model\n• Objects & services specified using an Interface Definition language (IDL)\n– Used to specify interface of objects and/or services\n• ORB: run-time system that handles object-client communication\n• Dynamic invocation interface: allows object invocation at run-time\n– Generic invoke operation: takes object reference as input\n– Interface repository stores all interface definitions\nCompsci 677: Distributed and OS Lec. 24 8\n\nObject Invocation Models\nRequest type Failure semantics Description\nCaller blocks until a response is\nSynchronous At-most-once\nreturned or an exception is raised\nCaller continues immediately without\nOne-way Best effort delivery\nwaiting for any response from the\nserver\nDeferred Caller continues immediately and\nAt-most-once\nsynchronous can later block until response is\ndelivered\n• Invocation models supported in CORBA.\n– Original model was RMI/RPC-like\n– Current CORBA versions support additional semantics\nCompsci 677: Distributed and OS Lec. 24 9\nEvent and Notification Services (1)\n• The logical organization of suppliers and consumers of events, following\nthe push-style model. (PUB-SUB model)\nCompsci 677: Distributed and OS Lec. 24 10\n\nEvent and Notification Services (2)\n• The pull-style model for event delivery in CORBA.\nCompsci 677: Distributed and OS CS677: Distributed OS Lec. 24 11\nMessaging: Async. Method Invocation\n• CORBA's callback model for asynchronous method invocation.\nCompsci 677: Distributed and OS Lec. 24 12\n\nMessaging (2)\n• CORBA'S polling model for asynchronous method invocation.\nCompsci 677: Distributed and OS Lec. 24 13\nDCOM\n• Distributed Component Object Model\n– Microsoft’s object model (middleware)\n– Now evolved into .NET\nCompsci 677: Distributed and OS Lec. 24 14\n\nDCOM: History\n• Successor to COM\n– Developed to support compound documents\n• Word document with excel spreadsheets and images\n• Object linking and embedding (OLE)\n– Initial version: message passing to pass information between parts\n– Soon replaced by a more flexible layer: COM\n• ActiveX: OLE plus new features\n– No good consensus on what exactly does ActiveX contain\n– Loosely: groups capabilities within applications to support scripting, grouping of objects.\n• DCOM: all of the above, but across machines\nCompsci 677: Distributed and OS Lec. 24 15\nType Library and Registry\n• The overall architecture of DCOM.\n– Type library == CORBA interface repository\n– Service control manager == CORBA implementation repository\nCompsci 677: Distributed and OS Lec. 24 16\n\nMonikers: Persistent Objects\nStep Performer Description\n1 Client Calls BindMoniker at moniker\nLooks up associated CLSID and instructs SCM to create\n2 Moniker\nobject\n3 SCM Loads class object\n4 Class object Creates object and returns interface pointer to moniker\n5 Moniker Instructs object to load previously stored state\n6 Object Loads its state from file\n7 Moniker Returns interface pointer of object to client\n• By default, DCOM objects are transient\n• Persistent objects implemented using monikers (reference stored on disk)\n– Has all information to recreate the object at a later time\nCompsci 677: Distributed and OS Lec. 24 17\nDistributed Coordination\n• Motivation\n– Next generation of systems will be inherently distributed\n– Main problem: techniques to coordinate various components\n• Emphasis on coordination of activities between components\nCompsci 677: Distributed and OS Lec. 24 18\n\nIntroduction to Coordination Models\n• Key idea: separation of computation from coordination\n• A taxonomy of coordination models\n– Direct coordination\n– Mailbox coordination\n– Meeting-oriented coordination (publish/subscribe)\n– Generative (shared tuple space)\nCompsci 677: Distributed and OS Lec. 24 19\nJini Case Study\n• Coordination system based on Java\n– Clients can discover new services as they become available\n– Example: “intelligent toaster”\n– Distributed event and notification system\n• Coordination model\n– Bulletin board model\n– Uses JavaSpaces: a shared dataspace that stores tuples\n• Each tuple points to a Java object\nCompsci 677: Distributed and OS Lec. 24 20\n\nOverall Approach\n• The principle of exchanging data items between publishers and subscribers.\nCompsci 677: Distributed and OS Lec. 24 21\nOverview of Jini\n• The general organization of a JavaSpace in Jini.\nCompsci 677: Distributed and OS Lec. 24 22\n\nCommunication Events\n• Using events in combination with a JavaSpace\nCompsci 677: Distributed and OS Lec. 24 23\nProcesses (1)\n• A JavaSpace can be replicated on all machines. The dotted lines show the partitioning of the JavaSpace into subspaces.\na) Tuples are broadcast on WRITE\nb) READs are local, but the removing of an instance when calling TAKE must be broadcast\nCompsci 677: Distributed and OS Lec. 24 24\n\nProcesses (2)\n• Unreplicated JavaSpace.\na) A WRITE is done locally.\nb) A READ or TAKE requires the template tuple to be broadcast in order to find a tuple instance\nCompsci 677: Distributed and OS CS677: Distributed OS Lec. 24 25\nDistributed Data Processing\n• Big data processing framework\n• Hadoop / Map Reduce\n• Spark\n• material courtesy of Natl Inst of Computational Sciences/ ORNL / Baer, Begoli et. al\nCompsci 677: Distributed and OS Lec. 24 26\n\nBig Data Applications\n• Very large datasets, need to distribute processing of data sets\n• Parallelize data processing\nCompsci 677: Distributed and OS Lec. 24 27\nMapReduce Programming Model\n• Map Phase and Reduce Phase, connected by a shuffle\nCompsci 677: Distributed and OS Lec. 24 28\n\nHadoop Big Data Platform\n• Popular platform for processing large amounts of data\n• EcoSystem:\n• Storage managers : HDFS, HBASE, Kafka, etc.\n• Processing framework: MapReduce, Spark, etc.\n• Resource managers: Yarn, Mesos, etc.\nCompsci 677: Distributed and OS Lec. 24 29\nEcosystem\nCompsci 677: Distributed and OS Lec. 24 30\n\nEcosystem overview\n• General purpose framework: low level processing APIs\n• MapReduce, Spark, Flink\n• Abstraction frameworks: higher level abstractions for processing\n• Pig\n• SQL frameworks: allow data querying : Hive\n• Graph processing frameworks: Giraph\n• Machine learning frameworks: MLlib, Oyyx (standalone: TensorFlow)\n• Real-time/stream processing: Spark Streaming, Storm, Kafka\n• Cluster managers: YARN, Mesos (allocate machines to separate frameworks).\nCompsci 677: Distributed and OS Lec. 24 31\nSpark Platform\n• Ease of use: supports Java, Scala or Python\n• General: combines SQL, streaming, ML, graph processing\n• Faster due to in-memory RDDs\n• Compatibility: runds on Hadoop, standalone, etc\nCompsci 677: Distributed and OS Lec. 24 32\n\nSpark Architecture\n• Resilient Distributed Datasets: distributed memory\n• objects cached in RAM across a cluster\n• DAG execution engine : eliminates MapReduce multi-stage model\n• RDD Narrow transform: Map, Filter, Sample\n• RDD Wide transform: SortBy, ReduceBy, GroupBy, Join\n• Action: Collect, Reduce\nCompsci 677: Distributed and OS Lec. 24 33"
  },
  "Lec24_notes": {
    "url": "https://lass.cs.umass.edu/~shenoy/courses/677content/notes/spring25/Lec24_notes.pdf",
    "parsed_text": "CMPSCI 677 Distributed Operating Systems Spring 2025\nLecture 24: April 30\nLecturer: PrashantShenoyScribe: JeffMao,RiyaSingh(2023),NitiMangwani(2024),ShivamRaj(2025)\n24.1 NFS (contd)\n24.1.1 Recap\nNFShasaweakconsistencymodel. Wheneveraclientapplicationusermodifiesafile,thechangesgetwritten\nto the cache at the client machine and later on the client can send the changes to the server. Meanwhile, if\nthe server receives a request for the same file from some other user it will send stale content.\n24.1.2 Client Caching: Delegation\nFigure 24.1: Delegation\nNFS supports the concept of delegation as part of caching. The client receives a master copy of the file to\nwhich the client can make updates. Upon completion, the client can send the file back to the server. This\nis similar to the concept of upload/download model. Thus, the server is delegating the file to the client so\nthattheclientcanhavealocalcopy. Ifanotherclienttriestoaccessthefile,theserverrecallsthedelegation\ngiven to previous client. The previous client returns the file to the server and then the server uses the old\nmodel where multiple clients can access the file by read/write requests to the sever.\nQuestion: When does the server decide to delegate the file?\nAnswer: Since this feature is stateful, it is only present in version 4. If the server is serving only one client\nthen the server can delegate the file. Otherwise since the server is not the current owner of the file, the\nserver can not delegate and thus has to use the old model. For example, files in the user’s home directory\ncan be delegated, whereas binaries of application programs can not be delegated as multiple users might\naccess them.\nQuestion: Is there a way to periodically update the server as in case of client failure the files may get lost?\nAnswer: It is possible for the client to flush the changes to the server in the background while it still holds\nthe master copy.\n24-1\n\n24-2 Lecture 24: April 30\n24.1.3 RPC Failures\nFigure 24.2: RPC Failures\nFor RPCs being used over TCP, TCP will take care of retransmissions between client and server. For RPCs\nover UDP, client and server can decide how to deal with lost requests and replies. Every RPC request is\nassociated with an ID. Upon receiving an RPC request from the client, the server will maintain the request\nand response for that request in its cache. If the client resends the request and the reply was lost, the server\nwill simply send the reply from the cache, instead of executing it again.\nQuestion: What is the utility of UDP over TCP?\nAnswer: UDP is faster than TCP as there is three-way handshake in TCP. In LANs, where probability of\nloss is low, RPCs can be sent over UDP. Over WANs or noisy LANs TCP may be preferred.\nQuestion: For how long can the reply be kept in the cache?\nAnswer: It depends on the application. Practically, after some unsuccessful tries within an hour, the client\nmay assume that the server is down. So the replies can be cached for some hours.\nQuestion: Is caching reply specific to some version of NFS?\nAnswer: It is not specific to some verison of NFS. In NFS v1, there was no concept of RPCs over TCP.\nThus,thismethodwasusedforRPCsoverUDP.Currently,withtheadventofRPCsoverTCP,thismethod\nis not needed to be used.\nQuestion: Is the cache needed only so that the requests are idempotent?\nAnswer: Yes. Forexample,ifrequestsarechangingfiles,itmightincurproblemsiftheyarenotidempotent\nand if requests are needed to be idempotent the cache is required.\n24.1.4 Security\nVersions 1, 2 and 3 of NFS relied on a simple security model. Every request is sent with user ID and\ngroup ID. The server checks for the file permissions on the basis of user ID and server ID. This ensures only\nauthenticated users can access the file. One drawback of this is that the channel between client and server,\nhowever, is not still secure. If an adversary intercepts the network traffic, the contents of a secure file may\nbe exposed. In version 4, the concept of secure RPCs was introduced. Every RPC client stub sends the\nrequest to the security layer which encrypts the request before sending. Thus, file contents can not be read\non the network.\nQuestion: Client can send user ID and group ID, but how does the server know if it is authentic?\nAnswer: As long as the server trusts the OS on the client the server knows the user ID and group ID are\nauthentic. However, if the OS is corrupted/hacked the server can not trust the client\n\nLecture 24: April 30 24-3\nFigure 24.3: Secure RPCs\n24.1.5 Replica Servers\nThere may be multiple servers serving different set of files. Version 4 allows the files to be replicated. Client\ncan make request for accessing files from any of the replicas. NFS provides implementation of maintaining\nconsistency between the replicated servers.\n24.2 Coda Overview\n24.2.1 DFS designed for mobile clients\n- Nice model for mobile clients who are often disconnected\n• Use file cache to make disconnection transparent\n• At home, on the road, away from network connection\n24.2.2 Coda supplements file cache with user preferences\n- E.g., always keep this file in the cache\n- Supplement with system learning user behavior\n24.2.3 How to keep cached copies on disjoint hosts consistent?\n- In mobile environment, ”simultaneous” writes can be separated by hours/days/weeks\n\n24-4 Lecture 24: April 30\nQuestion: What is coda using a remote access model or an upload download model?\nAnswer: It’salittlebitofboth. Whenyou’reconnected,yourchangescanbeuploadedorsenttotheserver\nimmediately, but you always have a cash. So when you are disconnected, you’re essentially just working\nwith whatever files subcaste, in which case you it looks like an upload download model. So the answer is it\nactually depends on whether you’re the state of.\n24.2.4 File Identifiers\nFigure 24.4: Coda architecture\n• Each file in Coda belongs to exactly one volume. A volume could be a disk or a partition of a disk.\n– Volume may be replicated across several servers. Identifiers include volume ID and file handle.\n– Multiple logical(replicated) volumes map to the same physical volume\n– 96 bit file identifier = 32 bit RVID + 64 bit file handle\n24.2.5 Server Replication\nFigure 24.5: Server replication issues in Coda\n\nLecture 24: April 30 24-5\nAssumethereare3serversand2clientsconnectedoveranetwork. Inanidealsituation,theserverskeepthe\ncopies of the files consistent. If there is a partition in the network, the servers no more have the same copy\nofthe files. Whennetwork partitionisfixed, theserverstry to synchronize the files. Ifthefiles aredifferent,\ntheremaynotbeanyproblems. Problemarisesiftheserversaccessthesamefilesduetowrite-writeconflicts.\n• Use replicated writes: read-once write-all\n– Writes are sent to all AVSG(all accessible replicas)\n• How to handle network partitions?\n– Use optimistic strategy for replication\n– Detect conflicts using a Coda version vector\n– Example: [2, 2, 1] and [1, 1, 2] is a conflict =¿ manual reconciliation\nQuestion: What is the size of the version vector?\nAnswer: The number of entries in the version vector is equal to the number of servers that have the copy\nof the file.\nQuestion: If the file is being updated multiple times will the system keep incrementing the version?\nAnswer: It is possible. It will still give rise to the same kind of conflict.\nQuestion: What does manual reconciliation mean?\nAnswer: Itmeansthattheuserhastomanuallyresolvetheconflictsinthesamewayastheuserisrequired\nto resolve merge conflicts in Git.\n24.2.6 Disconnected Operation\nHoarding state means the client is connected to the server and is actively downloading files into cache based\non some prediction based on current usage of the user. Upon disconnecting the client is in emulation state.\nUpon reconnecting to the server, the client is in reintegration state. The clients merge its updates with\nserver’s updates.\nFigure 24.6: Disconnected operation in Coda\n• The state-transition diagram of a Coda client with respect to a volume.\n• Use hoarding to provide file access during disconnection.\n– Prefetch all files that may be accessed and cache(hoard) locally\n– if AVSG=0, go to emulation mode and reintegrate upon reconnection\n\n24-6 Lecture 24: April 30\n24.2.7 Transactional Semantics\n• Notwork partition: part of network isolated from rest\n– Allow conflicting operations on replicas across file partitions\n– Reconcile upon reconnection\n– Transactional semantics =¿ operations must be serializable\n∗ Ensure that operations were serializable after thay have executed\n– Conflict =¿ force manual reconciliation\n24.2.8 Client Caching\n• Cache consistency maintained using callbacks\n24.3 xFS\n24.3.1 Overview of xFS\nFigure 24.7: An example of nodes in xFS\n• Key Idea: fully distributed file system [serverless file system]\n– Remove the bottleneck of a centralized system\n• xFS: x in ”xFS” = no server\n• Designed for high-speed LAN environments\nXFS combines two main concepts ; RAID - Redundant Array of Inexpensive Disks) and Log Structured\nFile Systems (LFS). It uses a concept of Network Stripping and RAID over a network wherein, a file is\npartitioned into blocks and provided to different servers. These blocks are then made as a Software RAID\nfile by computing a parity for each block which resides on a different machine.\nIn log structured File systems, data is sequentially written in the form of a log. The motivation for LFS\nwould be the large memory caches used by the OS. Larger, the size of cache, more the number of cache hits\ndue to reads, better will be the payoff due to the cache. The disk would be accessed only if there is a cache\nmiss. Due to the this locality of access, mostly write requests would trickle to the disk. Hence, the disk\ntraffic comes predominantly from write. In traditional hard drive disks, a disk head read or writes data .\n\nLecture 24: April 30 24-7\nHence, to read a block, a seeks needs to be done i.e. move the head to the right track on the disk.\nHow to optimize a file system which sees mostly write traffic ?\nThe basic insight is to reduce the time spent on seek and waiting for the required block to spin by. Every\nread/write request incurs a seek time and a rotational latency overhead. In general , random access layout\nis assumed for all blocks in the disk wherein the next block is present in an arbitrary location. This would\nrequire a seek time.\nTo eliminate this, a sequential form of writing facilitated by LFS can be used. The main idea of LFS is\nthat we try to write all the blocks sequentially one after the other. Thus LFS essentially buffers the writes\nand writes them in contiguous blocks into segments in a log like fashion. This will dramatically improve the\nperformance. Anynewmodificationwouldbeappendedattheendofthecurrentlogandhence, overwriting\nis not allowed. Any LFS requires a garbage collection mechanism to de-fragment and clean holes in the log.\nHence, XFS ensures 1. fault tolerance - due to RAID, 2. Parallelism - due to blocks being sent to multiple\nnodes. 3. High Performance - due to Log structured organization.\nIn SSD’s, the above mentioned optimization to log structures doesn’t give any benefits since there are no\nmoving parts and hence, no seek.\nQuestion: Is there an overhead to maintain lookup as block of the files need to be tracked?\nAnswer: There is higher overhead to maintain the lookup. For every write, the data gets appended, so it\nis meant to be for high write workloads. Metadata of the files is also written to the log. In case of lookups,\nthe metadata has to be accessed. Hence there is high overhead.\nQuestion: Can the writes be cached?\nAnswer: Reads are directly cached. Writes are cached in batches i.e. a batch of writes are written as an\nappend-only log.\nQuestion: Is LFS one server?\nAnswer: LFS are traditionally designed as single disk system. Here, they are combined with xFS. The logs\nare stripped across machines.\n24.3.2 xFS Summary\n• Distributes data storage across disks using software RAID and log-based network striping\n-RAID = Redundant Array of Independent Disks\n• Dynamically distribute control processing across all servers on a per-file granularity\n- Utilizes serverless management scheme.\n• Eliminates central server caching using cooperative caching\n- Harvest portions of client memory as a large, global file cache.\n24.3.3 Array Reliability\n• Reliability of N disks = Reliability of I Disk ÷ N\n50, 000 Hours ÷ 70 disks = 700 hours\nDisk system MTTF:Drops from 6 years to I month!\n\n24-8 Lecture 24: April 30\n• Arrays(without redundancy) too unreliable to be useful!\n24.4 RAID\n24.4.1 RAID Overview\n• Basic idea: files are ”striped” across multiple disks\n• Redundancy yields high data availability\n- Availability: service still provided to user, even if some components failed\n• Disks will still fail\n• Contents reconstructed from data redundantly stored in the array\n- Capacity penalty to store redundant info\n- Bandwidth penalty to update redundant info\n24.4.1.1 RAID\nRAID stands for Redundant Array of Independent Disks. In RAID based storage, files are striped across\nmultiple disks. Disk failures are to be handled explicitly in case of a RAID based storage. Fault tolerance is\nbuilt through redundancy.\nFigure 24.8: Striping in RAID\nFigure24.8showshowfilesarestoredinRAID.d1,...d4aredisks. Eachfileisdividedintoblocksandstored\nin the disks in a round robin fashion. So if a disk fails, all parts stored on that disk are lost. It has an\nadvantage that file can be read in parallel because data is stored on multiple disks and they can be read at\nthe same time. Secondly, storage is load balanced. If a file is popular and is requested more often, the load\nis evenly balanced across nodes. This also results in higher throughput.\nA disadvantage of striping is failure of disks. The performance of this system depends on the reliability of\ndisks. A typical disk lasts for 50,000 hours which is also knows as the Disk MTTF. As we add disks to the\nsystem, the MTTF drops as disk failures are independent.\nReliability of N disks=Reliability of 1 disk÷N\n\nLecture 24: April 30 24-9\nWe implement some form of redundancy in the system to avoid disadvantages caused by disk failures.\nDepending on the type of redundancy the system can be classified into different groups:\n24.4.1.2 RAID 1 (Mirroring)\nFrom figure 24.9, we can see that in RAID 1 each disk is fully duplicated. Each logical write involves two\nphysical writes. This scheme is not cost effective as it involves a 100% capacity overhead.\nFigure 24.9: RAID 1\n24.4.1.3 RAID 4\nFigure 24.10: RAID 4\nThismethodusesparitypropertytoconstructECC(ErrorCorrectingCodes)asshowninFigure24.10. First\na parity block is constructed from the existing blocks. Suppose the blocks D , D , D and D are striped\n0 1 2 3\nacross 4 disks. A fifth block (parity block) is constructed as:\nP =D ⊕D ⊕D ⊕D (24.1)\n0 1 2 3\nIf any disk fails, then the corresponding block can be reconstructed using parity. For example:\nD =D ⊕D ⊕D ⊕P (24.2)\n0 1 2 3\nThis error correcting scheme is one fault tolerant. Only one disk failure can be handled using RAID 4. The\nsize of parity group should be tuned so as there is low chance of more than 1 disk failing in a single parity\ngroup.\nQuestion: Where is the information about which files are on which disk?\nAnswer: The hardware controller serves the request internally to identify which blocks are stored on which\ndisk.\n\n24-10 Lecture 24: April 30\nQuestion: InRAID,hardwarecontrollerkeepsatrackofdatablocksandparity,whathappensifcontroller\nfails?\nAnswer: There will be problems in accessing the disk. That may be a point of failure. In case of Software\nRAID this issue will not occur.\nQuestion: Won’t the cost of accessing files increase since all disks are being accessed?\nAnswer: There are two ways to access a file, either block by block or accessing the whole file. If a request\nis made to access the whole file, in the above figure, eight requests to different disks would have been made,\nmaking it parallel. If the entire file would have been saved on the same disk, it would have resulted in eight\nrequests being made to the single disk, which makes it sequential. Thus, accessing multiple disks does not\nnecessarily make it expensive.\n24.4.1.4 RAID 5\nFigure 24.11: RAID 5\nOne of the main drawbacks of RAID 4 is that all parity blocks are stored on the same disk. Also, there are\nk + 1 I/O operations on each small write, where k is size of the parity block. Moreover, load on the parity\ndiskissumofloadonotherdisksintheparityblock. Thiswillsaturatetheparitydiskandslowdownentire\nsystem.\nIn order to overcome this issue, RAID 5 uses distributed parity as shown in Figure 24.11. The parity blocks\nare distributed in an interleaved fashion.\nNote: All RAID solutions have some write performance impact. There is no read performance impact.\n\nLecture 24: April 30 24-11\nRAIDimplementationsaremostlyonhardwarelevel. HardwareRAIDimplementationaremuchfasterthan\nsoftware RAID implementations.\n24.4.2 xFS uses software RAID\n• Two limitations\n– Overhead of parity management hurts performance for small writes\n∗ Ok, if overwriting all N-1 data blocks\n∗ Otherwise, must read old parity+data blocks to calculate new parity\n∗ Small writes are common in UNIX-like systems\n– Very expensive since hardware RAIDS add special hardware to compute parity\n24.4.3 Log-structured FS\n• Provide fast writes, simple revovery, flexible file location method\n• Key Idea: buffer writes in memory and commit to disk in large, contiguous, fixed-size log segments\n– Complicates reads, since data can be anywhere\n– Use per-file inodes that move to the end of the log to handle reads\n– Uses in-memory imap to track mobile inodes\n∗ Periodically checkpoints imap to disk\n∗ Enables ”roll forward” failure recovery\n– Drawback: must clean ”holes” created by new writes\n24.4.4 Combine LFS with Software RAID\nLogwrittensequentiallyarechoppedintoblockswhichaparitygroups. Eachparitygroupbecomesaserver\non a different machine in a RAID fashion\n24.5 HDFS - Hadoop Distributed File system\nIt is designed for high throughput - very large datasets. It optimizes the data for batch processing rather\nthaninteractiveprocessing. HDFShasasimplecoherencymodelinwhichitassumesaWORM(WriteOnce\nRead Many) model. In WORM, file do not change and changes are append-only.\n24.5.1 Architecture\nThere are 2 kinds of nodes in HDFS ; Data and Meta-data nodes. Data nodes store the data whereas,\nmeta-datakeepstrackofwherethedataisstored. Averageblocksizeinafilesystemis4KB.InHDFS,due\nto large datasets, block size is 64 MB.Replication of data prevents disk failures. Default replication factor\nin HDFS is 3.\n\n24-12 Lecture 24: April 30\n24.6 GFS - Google File System\nMasternode actsas ameta-data server. Ituses afile systemtree tolocate thechunks (GFSterminology for\nblocks). Each chunk is replicated on 3 nodes. Each chunk is stored as a file in Linux file system.\n24.7 Object Storage Systems\n• Use handles(e.g., HTTP) rather than files names\n– Location transparent and location independence\n– Separation of data from metadata\n• No block storage: objects of varying sizes\n• Uses\n- Archival storage\n– can use internal data de-duplication\n- Cloud Storage: Amazon S3 service\n– uses HTTP to put and get objects and delete\n– Bucket: objects belong to bucket/partitions name space\n24.8 Distributed Objects\nIn case of remote objects, code on a client machine wants to invoke an object’s method on a server machine.\nA common way to achieve this, is as follows. Clients have a stub called proxy with an interface matching\nthe remote object. An invocation of a proxy’s method is passed across the network to the ’skeleton’ on the\nserver. That skeleton invokes the method on the remote object and returns the marshalled response. This\ncan be recognized from earlier in the course as an RMI or RPC call.\nDistributed objects are similar, but the distributed objects are themselves partitioned or replicated across\ndifferent machines. Distributed objects use RPC. Middleware systems have been developed to support\ndistributed objects.\n24.9 Enterprise Java Beans\nEnterprise Java is used to write multi-tier applications where the app server is actually written in Java. It\nalso gives some additional functionality like the concept of a bean. A bean is a special type of an object.\nAs a middleware, we will essentially have our objects written as a bean of some sort. It also provides other\nserviceslikeRMI,JNDI,JDBC(usedtoconnecttoDatabases),JMS(JavaMessagingService). EJBssupport\nmore functionality that makes it easier to write web applications.\nEJB are fundamentally object oriented, with two components, the interface and the implementation. The\nEJB class encodes the business logic of the application. EJB helps in persisting state of objects to the disk\nand retrieve when needed.\n\nLecture 24: April 30 24-13\n24.9.1 Four Types of EJBs\n• Stateless session beans-Theyareessentiallyobjectswhichdonothaveanystateatall,theymight\njust expose code.\n• Stateful session beans - By default, the memory state is transient and if we kill the application,\nthe object is gone. We can automatically persist the state of the object using these. Two important\nattributes: 1. Session 2. Session state is stored on the server side.\n• Entity beans - They look more like standard Java objects. The object has state which is persisted\non disk\n• Message-driven beans - They are designed for messaging and the messages can persist.\n24.10 CORBA : Common Object Request Broker Architecture\nAt the core of CORBA is the Object Request Broker (ORB) [also called messaging bus] is a intermediate\ncommunication channel that allows communication between objects.\nFigure 24.12: CORBA\nThe four boxes are the four components and they communicate using RPCs handled by the ORB. Many\nfunctionalities are already provided in CORBA as a service. They provide a dozen different services from\nconcurrency to licensing in complex distributed systems. The advantage is that they can help reduce the\ncode needed to develop complex distributed systems. However, in trying to provide every service you’d\never need, CORBA became very heavy weight. It does a lot of overhead to write even a small application.\nBy becoming very heavy-weight, it became very difficult to learn and simple distributed applications would\nrequire deploying such a heavy weight system. Even though it did not become a commercial success, some\nstrippeddownversionsofCORBAactuallygotused. Example-messagingserviceinLinuxdesktopmanager\ncalled gnome. EJB are widely used.\nThe stub in the object model of CORBA is called ORB. It uses Interface Definition Language (IDL) to\nuse an interface and compiler to generate code (like protobufs). Proxy is used to specify the objects and\nservices. Object adapter provides portability between languages. Thus CORBA, is language independent.\nIt also allows dynamic invocation of interfaces. At runtime, CORBA can fetch interfaces to put in the stub\nwhich can then be used. CORBA provided even more flexibility having the option of invoking RPCs as\nany type including synchronous, one-way, or deferred synchronous. CORBA was one of the first distributed\nmiddleware systems. Modern middleware systems take many ideas from it.\n24.10.1 Event and Notification Services\nThisisdoneusinganeventchannel. Itallowsustoimplementanapplicationusingthepublisher-subscriber\nmodel. Publishers post events to the event channel, and consumers/subscribers ask for events that they\nsubscribe to from the event channel. Publisher subscriber works with any combination of push and pull. In\n\n24-14 Lecture 24: April 30\nCORBA, it is a push-push model where data is pushed from publisher to event channel. The event channel\nwill see the list of consumers subscribed and whenever there is a match, it will push to the consumer. Event\nchannel can be thought of as a buffer. In pull-pull model, event channel polls data from the publisher and\nsimilarly, consumer pulls data from the event channel.\nTwo ways to implement the event channel: 1. Push based model. 2. Pull based model.\nWe can have hybrid models as well where we can have combination of push and pull models.\nFigure 24.13: Event and notification services (1)\nFigure 24.14: Event and notification services (2)\nQuestion: Can this be used to check cache invalidation?\nAnswer: Yes, if you want to do cache invalidation, the cache could subscribe to events, and specify what\ndata they have, and the server could push events when data is exchanged.\n24.10.2 Messaging - Async method Invocation\nToimplementallkindsofRPCs, CORBAhascallbackmodel. Itwillhaveacallbackinterfacewherewecan\nregister a callback function. Whenever a reply to the async RPC comes back, we are notified and we can\nget our reply. We can also use the polling method where we keep polling periodically to see if the reply has\ncome back.\n24.10.3 Messaging - Polling based model\nIn this, the consumer keeps polling the event channel to check if there is any data that aligns with the\nsubscription.\n\nLecture 24: April 30 24-15\n24.11 DCOM : Distributed Component Object Model\nDCOMisMicrosoft’smiddlewarewhichhasnowevolvedinto.NET.DCOMwillonlyrunonWindowsservers\nordesktops. COMisasimpleRMIbasedframeworkrunninglocaltoamachinethatallowedcommunication\nwithin a machine. It is mainly used for communication between Microsoft applications. Object Linking and\nEmbedding (OLE) was added to allow Microsoft office applications to communicate with one another via\nembedding and document linking. The ActiveX layer facilitates exposing these services as web applications\nby allowing us to embed things in web documents. Microsoft picked up this whole thing and made it\ndistributedcalledDCOM..NEThasalanguageindependentruntime, butActiveXonlyworkswithInternet\nExplorer.\nThe architecture of DCOM is fundamentally the same as the distributed objects. The stub is essentially the\nCOM layer. At its core, it is an RMI based system on objects. There is a type library which is similar to\nCORBA’s interface library. We also have a SCM (Service Control Manager) which keeps track of what all\nobjects are in the system and where are they running. DCOM is not as heavy as CORBA.\nThe objects in DCOM can also be made persistent even though they are transient by default. It is done\nusing the notion of a Moniker. Moniker is the name of a persistent object that allows us to reconstruct that\nobject after we shut down the server application.\nFigure 24.15: DCOM\n24.12 Distributed Coordination Middleware\nIn this case, we have a very loose coupling between how communication works. The idea is that we want\nto separate our computations from coordination. Distributed applications can be classified based on whats\nhappening in time and space dimension. Applications can either be coupled or decoupled in space and time.\n1. ⟨ coupled in space and time ⟩ Direct\n2. ⟨ coupled in space but not time ⟩ Mailbox - receiver is known but receiver state does not matter.\n\n24-16 Lecture 24: April 30\n3. ⟨ coupled in time but not space ⟩ Meeting Oriented - unknown who will show up to meeting.\n4. ⟨ decoupled in space and time ⟩ Generative Communication - components can communicate with\nanother without knowing who might read it or when it would be read. It uses a pub-sub model.\nQuestion: If the applications are coupled in time, how does the clock synchronization work?\nAnswer: Coupledintimedoesnotnecessitatethatclockshavetobesynchronized. Itjustmeansthatboth\nparties need to be active at the same time. It could be two people or two processes.\n24.12.1 Jini Case Study\nJiniisaJavabasedmiddlewarethatusesdistributedcoordination. Itfacilitatesservicediscovery. Wedonot\nknow what entities are present in the system so this notion of discovery allows us to discover what services\nare available and so on. These are also called zero-configuration services because we do not need to pre\nconfigure anything as services as discovered on the fly. It uses a event notification system that is pub-sub\nbased. Jini uses a bulletin board architecture. Services advertise on the bulletin board and machines can\naccess the services it requires through the board. It is decoupled in time and space.\nIn Jini, bulletin board is called JavaSpace or tuple space. JavaSpace is basically a database. Each tuple is a\nJava object. We have reads and writes into a shared database. One can request callbacks in this model as\nwell.\nInstructor’s Example: You take your laptop and visit your friend. You connect to their Wifi network.\nNow, you want to print a document and they have a printer in their house. What is the normal process you\nwould have to go through to print the document? Answer: You will have to configure the printer.\nInthiscase, yourmachinecandiscoverwhatprintersareavailable. Now, youwillbeshownalistofprinters\nand then you can choose and it will configure itself and print. Nothing to be done manually.\nQuestion : How is this different from a Publish-Subscribe Model?\nAnswer: InPub-Sub,subscriptionsaredonebeforehand. Inaddition,messagespostedinthebulletin-board\nmodel can be stored for a long period of time.\nQuestion : Where is this JavaSpace running?\nAnswer : JavaSpace is our middleware service which has to run on some server or some set of servers. So\nessentially, our middleware is running somewhere else and we have to read and write from that.\nJini utilizes a pub-sub architecture with both pull based as well as notification based discovery. Here the\nmessages are persisted on disk unlike Event channel where messages don’t persist.\nHere, JavaSpace is distributed across multiple machines. The boxes are JavaSpaces with tuples in them.\nJavaSpaces can either be fully replicated or distributed. In case of replicated JavaSpaces, writes need to be\nbroadcasted to all replicas whereas reads are local. On the other hand, in distributed bulletin board, each\nboard has a subset of nodes, while writes are local and reads need to be done on each and every board.\nQuestion: WhateverwearepostingintheJavaSpace,isitaJavaobjectandhowisitposted(copied/sent)?\nAnswer : Tuples are not Java objects, they are data objects like a key and a value. So essentially, we are\npublishing data or events instead of objects which is different from sending/ receiving objects.\nQuestion: Why can there be multiple copies of data on the bulletin board?\nAnswer: That was just shown as a logical view. In reality, if the board is replicated, we write it to all the\nboards. The logical view just shows a union of all replicas\n\nLecture 24: April 30 24-17\nFigure 24.16: Jini Processes\nQuestion: Since messages are not intended for any one process, how does this process(in the example) who\nread C know that it has to delete C from the database?\nAnswer: It depends on the number of recipients of a message. If its just one recipient, then the first time\na recipient comes and looks for C, we can delete C. If many recipient, then we wont remove it.\n24.13 Distributed Middleware Systems\nThese are middleware systems that are designed for large scale data processing.\n24.13.1 Big Data Applications\nThis is mostly covered in CS532 systems for data science. So we will only partially cover this. We have to\nparallelized data processing using distributed systems as its a large amount of data.\nDistributed data processing is different types of middleware that are designed for processing large amounts\nof data. The basic idea is that we will use multiple machines of a cluster and parallelize our application for\ndata processing. Each machine will read and process some part of the data.\n24.13.2 MapReduce Programming Model\nMapReduceisaframeworkforprocessingparallelizableproblemsacrosslargedatasetsusingalargenumber\nof computers (nodes), collectively referred to as a cluster (if all nodes are on the same local network and use\nsimilar hardware) or a grid (if the nodes are shared across geographically and administratively distributed\nsystems, and use more heterogenous hardware). Processing can occur on data stored either in a filesys-\ntem (unstructured) or in a database (structured). MapReduce can take advantage of the locality of data,\nprocessing it near the place it is stored in order to minimize communication overhead.\nMapReduce is a two-stage process to process a large dataset - Map phase and Reduce phase.\n\n24-18 Lecture 24: April 30\nFigure 24.17: Map Reduce example\nMap Step : Each worker node applies the ”map()” function to the local data, and writes the output to a\ntemporary storage. A master node ensures that only one copy of redundant input data is processed.\nShuffle Step : Worker nodes redistribute data based on the output keys (produced by the ”map()” func-\ntion), such that all data belonging to one key is located on the same worker node.\nReduce Step : Worker nodes now process each group of output data, per key, in parallel.\nExample1: Let’s say we want to do word counting on a large set of documents. A chunk of data is given to\neach machine which counts the words within that chunk (map phase). Now we need to sum these up. This\nisdoneinreducephase-1nodeisassignedtoeachword. Itreceivescountfrommachinesforthatwordand\nsums them up.\nExample2: Let’s say we have huge dataset of number (or may be words in a document) which need to be\nsorted (or frequency of words). Suppose we have multiple machines. Then each machine could take a chunk\nof data and sort it. Later, all these sorted chunks chould be merge together similar to merge sort algorithm.\nHowever, this requires huge communication between the machines during merge phase. To overcome this\nproblem, let’s say we know the range of numbers. In such a case, we could follow the bucket sort paradigm\nwhere each machine sorts a specific range of numbers. This way, inter-machine communication can be\nreduced.\nThe datasets being processed here are actually stored on HDFS. Each node can read its local data from\nHDFS or it can also read remote data. Reading locally has lesser overheads and is cheaper.\nQuestion: What is the reason to shuffle? Why cant you serialize it?\nAnswer: That is what we are doing. In the second phase, we have to decide who is responsible for which\nset of data and send it to that node. Because all the data incoming to the first phase (Node 1) in the case\nof sorting a data is random. So from second phase we decide what set of data will be handled by which set\nof nodes and forward it.\n24.13.3 Hadoop Big Data Platform\nHadoopisanimplementationofMap-Reduceframework. Itisanopen-sourcesoftwareframeworkforstoring\ndata and running applications on clusters of commodity hardware. It provides massive storage for any kind\n\nLecture 24: April 30 24-19\nof data, enormous processing power and the ability to handle virtually limitless concurrent tasks or jobs. It\nhas :\n• store managers: wherethedatasetsarestored. Eg: HDFS,HBASE,Kafka,etc(replicationforfault\ntolerence.\n• processing framework : Map-reduce, Spark, etc\n• resource managers : allocates nodes and resources to jobs. Some of the concepts of distributed\nscheduling are also adopted since they need to serve multiple users. Example : Yarm, Mesos, etc\nQuestion : How is HBASE using MapReduce?\nAnswer : HBASE is just a storage layer. MapReduce reads from the storage layer, in this case - HBASE\n24.13.3.1 Ecosystem\nBased on the requirements different types of frameworks can be used. For example, if user wants to process\nthe data that have lot of graphs then Graph processing framework Giraph can be used. There are machine\nlearning frameworks like MLLib, Oyyx, Tensorflow that are also designed to run to on Hadoop. If a user\nwants to input data to these distributed processing framework, he could use applications like hive to easily\nwritemap-reducecodes. Forrealtimedataprocessingwheredataisgeneratedcontinuouslybysomeexternal\nsource, framework like Spark Storm etc could be used.\nWe can see that it is not a single distributed application, it is a set of applications that work together.\n24.13.4 Spark Platform\nApache Spark is a powerful open source processing engine built around speed, ease of use, and sophisticated\nanalytics. It was originally developed at UC Berkeley in 2009. Spark was an important innovation over\nMapReduce. Although MapReduce uses parallelism, it is very heavy on I/O that can slow down the appli-\ncation. In Spark, we store intermediate data in memory of some server. What we decide to store and how\nto store is something we have to think about when writing a Spark application.\nFigure 24.18: Spark Platform\nSpark SQL : Spark SQL is a Spark module for structured data processing. It provides a programming\nabstractioncalledDataFramesandcanalsoactasdistributedSQLqueryengine. Itenablesunmodified\nHadoop Hive queries to run up to 100x faster on existing deployments and data. It also provides\npowerfulintegrationwiththerestoftheSparkecosystem(e.g., integratingSQLqueryprocessingwith\nmachine learning).\n\n24-20 Lecture 24: April 30\nSpark Streaming Many applications need the ability to process and analyze not only batch data, but\nalso streams of new data in real-time. Running on top of Spark, Spark Streaming enables powerful\ninteractive and analytical applications across both streaming and historical data, while inheriting\nSpark’s ease of use and fault tolerance characteristics. It readily integrates with a wide variety of\npopular data sources, including HDFS, Flume, Kafka, and Twitter.\nMLlib Built on top of Spark, MLlib is a scalable machine learning library that delivers both high-quality\nalgorithms (e.g., multiple iterations to increase accuracy) and blazing speed (up to 100x faster than\nMapReduce). The library is usable in Java, Scala, and Python as part of Spark applications, so that\nyou can include it in complete workflows.\nGraphX GraphX is a graph computation engine built on top of Spark that enables users to interactively\nbuild, transform and reason about graph structured data at scale. It comes complete with a library of\ncommon algorithms.\nAdavatages of Spark\n• Speed : Run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on\ndisk.Apache Spark has an advanced DAG execution engine that supports acyclic data flow and in-\nmemory computing.\n• Ease of Use: Spark offers over 80 high-level operators that make it easy to build parallel apps. And\nyou can use it interactively from the Scala, Python and R shells.\n• Generality : Combine SQL, streaming, and complex analytics.Spark powers a stack of libraries\nincluding SQL and DataFrames, MLlib for machine learning, GraphX, and Spark Streaming. You can\ncombine these libraries seamlessly in the same application.\n• Runs Everywhere: Spark runs on Hadoop, Mesos, standalone, or in the cloud. It can access diverse\ndata sources including HDFS, Cassandra, HBase, and S3.\n24.13.4.1 Spark Architecture\nDistributed memory is just like memory but we access data across various machines. All of the memories\nof the servers can be accessed if we store data in the form of an RDD (Resilient Distributed Dataset). The\nidea is that we will first read data from disk, say HDFS. Then we will do some partial processing, then\ntransformed dataset will be stored in RDD and so on. Since data is in memory, processing will be much\nfaster. If the data is larger than the memory available, data can spill over to disk.\nRDDisdesignedtosupportin-memorydatastorage,distributedacrossaclusterinamannerthatisdemon-\nstrably both fault-tolerant and efficient. Fault-tolerance is achieved, in part, by tracking the lineage of\ntransformations applied to coarse-grained sets of data. Efficiency is achieved through parallelization of pro-\ncessingacrossmultiplenodesinthecluster,andminimizationofdatareplicationbetweenthosenodes. Server\nfailures can be handled by recomputation.\nQuestion : Is Spark a type of distributed middleware?\nAnswer : Both Hadoop and Spark are type of distributed middleware designed for large data processing.\nQuestion : In Spark where are the computations stored?\nAnswer: Thecomputationsarecodethatauserhaswritten. Sparkwillkeeptrackofgraphasitsgenerated.\nFor each node it will keep track of what code was run to generate this output. We will only redo that part\nto do the minimum computation to generate the data."
  },
  "Lec23_slides": {
    "url": "https://lass.cs.umass.edu/~shenoy/courses/677content/slides/spring25/Lec23.pdf",
    "parsed_text": "Today: Coda, xFS\n• Distributed File Systems\n• Case Study: Coda File System\n• Brief overview of other file systems\n• xFS\n• Log structured file systems\n• HDFS\n• Object Storage Systems\nCompsci 677: Distributed and OS Lec. 23 1\nCoda Overview\n• DFS designed for mobile clients\n• Nice model for mobile clients who are often disconnected\n• Use file cache to make disconnection transparent\n• At home, on the road, away from network connection\n• Coda supplements file cache with user preferences\n• E.g., always keep this file in the cache\n• Supplement with system learning user behavior\n• How to keep cached copies on disjoint hosts consistent?\n• In mobile environment, “simultaneous” writes can be separated by hours/days/weeks\nCompsci 677: Distributed and OS Lec. 23 2\n\nFile Identifiers\n• Each file in Coda belongs to exactly one volume\n• Volume may be replicated across several servers\n• Multiple logical (replicated) volumes map to the same physical volume\n• 96 bit file identifier = 32 bit RVID + 64 bit file handle\nCompsci 677: Distributed and OS Lec. 23 3\nServer Replication\n• Use replicated writes: read-once write-all\n• Writes are sent to all AVSG (all accessible replicas)\n• How to handle network partitions?\n• Use optimistic strategy for replication\n• Detect conflicts using a Coda version vector\n• Example: [2,2,1] and [1,1,2] is a conflict => manual reconciliation\nCompsci 677: Distributed and OS Lec. 23 4\n\nDisconnected Operation\n• The state-transition diagram of a Coda client with respect to a volume.\n• Use hoarding to provide file access during disconnection\n• Prefetch all files that may be accessed and cache (hoard) locally\n• If AVSG=0, go to emulation mode and reintegrate upon reconnection\nCompsci 677: Distributed and OS Lec. 23 5\nTransactional Semantics\n• Network partition: part of network isolated from rest\n• Allow conflicting operations on replicas across file partitions\n• Reconcile upon reconnection\n• Transactional semantics => operations must be serializable\n• Ensure that operations were serializable after thay have executed\n• Conflict => force manual reconciliation\nCompsci 677: Distributed and OS Lec. 23 6\n\nOverview of xFS.\n• Key Idea: fully distributed file system [serverless file system]\n• Remove the bottleneck of a centralized system\n• xFS: x in “xFS” => no server\n• Designed for high-speed LAN environments\nCompsci 677: Distributed and OS Lec. 23 7\nxFS Summary\n• Distributes data storage across disks using software RAID and log-based\nnetwork striping\n• RAID == Redundant Array of Independent Disks\n• Dynamically distribute control processing across all servers on a per-file\ngranularity\n• Utilizes serverless management scheme\n• Eliminates central server caching using cooperative caching\n• Harvest portions of client memory as a large, global file cache.\nCompsci 677: Distributed and OS Lec. 23 8\n\nArray Reliability\n• Reliability of N disks = Reliability of 1 Disk ÷ N\n50,000 Hours ÷ 70 disks = 700 hours\nDisk system MTTF: Drops from 6 years to 1 month!\n• Arrays (without redundancy) too unreliable to be useful!\nHot spares support reconstruction in parallel with\naccess: very high media availability can be achieved\nCompsci 677: Distributed and OS Lec. 23 9\nRAID Overview\n• Basic idea: files are \"striped\" across multiple disks\n• Redundancy yields high data availability\n–Availability: service still provided to user, even if some components failed\n• Disks will still fail\n• Contents reconstructed from data redundantly stored in the array\n–Capacity penalty to store redundant info\n–Bandwidth penalty to update redundant info\nSlides courtesy David Patterson\nCompsci 677: Distributed and OS Lec. 23 10\n\nMirroring in RAID\nrecovery\ngroup\n• Each disk is fully duplicated onto its “mirror”\n• Very high availability can be achieved\n• Bandwidth sacrifice on write:\n• Logical write = two physical writes\n• Reads may be optimized\n• Most expensive solution: 100% capacity overhead\n• (RAID 2 not interesting, so skip…involves Hamming codes)\nCompsci 677: Distributed and OS Lec. 23 11\nInspiration for RAID 5\n• Use parity for redundancy\n• D0 ⨂ D1 ⨂ D2 ⨂ D3 = P\n• If any disk fails, then reconstruct block using parity:\n• e.g., D0 = D1 ⨂ D2 ⨂ D3 ⨂ P\n• RAID 4: all parity blocks stored on the same disk\n• Small writes are still limited by Parity Disk: Write to D0, D5, both also write to P disk\n• Parity disk becomes bottleneck\nD0 D1 D2 D3 P\nD4 D5 D6 D7 P\nCompsci 677: Distributed and OS Lec. 23 12\n\nRedundant Arrays of Inexpensive Disks RAID 5: High I/O Rate Interleaved\nParity\nIncreasing\nLogical\nDisk\nIndependent writes D0 D1 D2 D3 P Addresses\npossible because of\ninterleaved parity\nD4 D5 D6 P D7\nD8 D9 P D10 D11\nD12 P D13 D14 D15\nExample:\nwrite to D0, P D16 D17 D18 D19\nD5 uses disks\n0, 1, 3, 4\nD20 D21 D22 D23 P\n. . . . . . . . . .\n13\nDisk Columns\nCompsci 677: Distributed and OS Lec. 23 13\nxFS uses software RAID\n• Two limitations\n• Overhead of parity management hurts performance for small writes\n• Ok, if overwriting all N-1 data blocks\n• Otherwise, must read old parity+data blocks to calculate new parity\n• Small writes are common in UNIX-like systems\n• Very expensive since hardware RAIDS add special hardware to compute\nparity\nCompsci 677: Distributed and OS Lec. 23 14\n\nLog-structured FS\n• Provide fast writes, simple recovery, flexible file location method\n• Key Idea: buffer writes in memory and commit to disk in large, contiguous, fixed-size log\nsegments\n• Complicates reads, since data can be anywhere\n• Use per-file inodes that move to the end of the log to handle reads\n• Uses in-memory imap to track mobile inodes\n• Periodically checkpoints imap to disk\n• Enables “roll forward” failure recovery\n• Drawback: must clean “holes” created by new writes\nCompsci 677: Distributed and OS Lec. 23 15\nCombine LFS with Software RAID\n• The principle of log-based striping in xFS\n• Combines striping and logging\nCompsci 677: Distributed and OS Lec. 23 16\n\nHDFS\n• Hadoop Distributed File System\n• High throughput access to application data\n• Optimized for large data sets (accessed by Hadoop)\n• Goals\n• Fault-tolerant\n• Streaming data access: batch processing rather than interactive\n• Large data sets: scale to hundreds of nodes\n• Simple coherency model: WORM (files don’t change, append )\n• Move computation to the data when possible\nCompsci 677: Distributed and OS Lec. 23 17\nHDFS Architecture\n• Principle: meta data nodes separate from data nodes\n• Data replication: blocks size and replication factor configurable\nCompsci 677: Distributed and OS Lec. 23 18\n\nGoogle File System\n• Master-slave; file divided into chunks (replicated thrice)\n• Atomic writes\nCompsci 677: Distributed and OS Lec. 23 19\nObject Storage Systems\n• Use handles (e.g., HTTP) rather than files names\n• Location transparent and location independence\n• Separation of data from metadata\n• No block storage: objects of varying sizes\n• Uses\n• Archival storage\n• can use internal data de-duplication\n• Cloud Storage : Amazon S3 service\n• uses HTTP to put and get objects and delete\n• Bucket: objects belong to bucket/ partitions name space\nCompsci 677: Distributed and OS Lec. 23 20"
  },
  "Lec23_notes": {
    "url": "https://lass.cs.umass.edu/~shenoy/courses/677content/notes/spring25/Lec23_notes.pdf",
    "parsed_text": "CMPSCI 677 Distributed Operating Systems Spring 2025\nLecture 23: April 28\nLecturer: Prashant Shenoy Scribe: Sylvia Imanirakiza (2025), Chaitali Agarwal (2024)\n23.1 NFS (contd)\n23.1.1 Recap\nNFShasaweakconsistencymodel. Wheneveraclientapplicationusermodifiesafile,thechangesgetwritten\nto the cache at the client machine and later on the client can send the changes to the server. Meanwhile, if\nthe server receives a request for the same file from some other user it will send stale content.\n23.1.2 Client Caching: Delegation\nFigure 23.1: Delegation\nNFS supports the concept of delegation as part of caching. The client receives a master copy of the file to\nwhich the client can make updates. Upon completion, the client can send the file back to the server. This\nis similar to the concept of upload/download model. Thus, the server is delegating the file to the client so\nthattheclientcanhavealocalcopy. Ifanotherclienttriestoaccessthefile,theserverrecallsthedelegation\ngiven to previous client. The previous client returns the file to the server and then the server uses the old\nmodel where multiple clients can access the file by read/write requests to the sever.\nQuestion: When does the server decide to delegate the file?\nAnswer: Since this feature is stateful, it is only present in version 4. If the server is serving only one client\nthen the server can delegate the file. Otherwise since the server is not the current owner of the file, the\nserver can not delegate and thus has to use the old model. For example, files in the user’s home directory\ncan be delegated, whereas binaries of application programs can not be delegated as multiple users might\naccess them.\nQuestion: Is there a way to periodically update the server as in case of client failure the files may get lost?\nAnswer: It is possible for the client to flush the changes to the server in the background while it still holds\nthe master copy.\n23-1\n\n23-2 Lecture 23: April 28\n23.1.3 RPC Failures\n23.2 Coda Overview\n23.2.1 DFS designed for mobile clients\n- Nice model for mobile clients who are often disconnected\n• Introduced in late 80-90s by CMU\n• Use file cache to make disconnection transparent\n• Designed for weakly connected devices which can be used at home, on the road, away from network\nconnection\n• Serves as a precursor to Cloud drives\n• It supplements file cache with user preferences. E.g. always keep this file in the cache or Supplement\nwith system learning user behavior.\n• Uses replicated writes: Read once and write all. Writes are sent to all accessible replicas.\nQuestion: Is coda using a remote access model or an upload download model?\nAnswer: It’salittlebitofboth. Whenyou’reconnected,yourchangescanbeuploadedorsenttotheserver\nimmediately, but you always have a cache. So when you are disconnected, you’re essentially just working\nwith files caches, in which case you it looks like an upload download model. So the answer is it actually\ndepends on whether you’re the state of the mobile device.\n23.2.2 File Identifiers\nFigure 23.2: Coda architecture\n• Each file in Coda belongs to exactly one volume. A volume could be a disk or a partition of a disk.\n\nLecture 23: April 28 23-3\n– Volume may be replicated across several servers. Identifiers include volume ID and file handle.\n– Multiple logical(replicated) volumes map to the same physical volume\n– 96 bit file identifier = 32 bit RVID + 64 bit file handle\n– Each write increments the version number. Similar to versions maintained by git.\n23.2.3 Server Replication\nFigure 23.3: Server replication issues in Coda\nAssumethereare3serversand2clientsconnectedoveranetwork. Inanidealsituation,theserverskeepthe\ncopies of the files consistent. If there is a partition in the network, the servers no more have the same copy\nofthe files. Whennetwork partitionisfixed, theserverstry to synchronize the files. Ifthefiles aredifferent,\ntheremaynotbeanyproblems. Problemarisesiftheserversaccessthesamefilesduetowrite-writeconflicts.\n• Use replicated writes: read-once write-all\n– Writes are sent to all AVSG(all accessible replicas)\n• How to handle network partitions?\n– Use optimistic strategy for replication\n– Detect conflicts using a Coda version vector\n– Example: [2, 2, 1] and [1, 1, 2] is a conflict: manual reconciliation\nQuestion: What is the size of the version vector?\nAnswer: The number of entries in the version vector is equal to the number of servers that have the copy\nof the file.\nQuestion: If the file is being updated multiple times will the system keep incrementing the version?\nAnswer: It is possible. It will still give rise to the same kind of conflict.\nQuestion: What does manual reconciliation mean?\nAnswer: Itmeansthattheuserhastomanuallyresolvetheconflictsinthesamewayastheuserisrequired\nto resolve merge conflicts in Git.\n\n23-4 Lecture 23: April 28\n23.2.4 Disconnected Operation : Client disconnects from Server\n• Hoarding State: Client is connected to the server and is actively downloading files into cache based\non some prediction based on current usage of the user. The client tries to cache copies that the user is\nlikely to access.\n• Emulation State: When the client is disconnected from the server/internet and uses the cached\ncopies.\n• Reintegration State: Client is connected to the internet/server and merges its updates with the\nserver.\nFigure 23.4: Disconnected operation in Coda\n• The state-transition diagram of a Coda client with respect to a volume.\n• Use hoarding to provide file access during disconnection.\n– Prefetch all files that may be accessed and cache(hoard) locally\n– if AVSG=0, go to emulation mode and reintegrate upon reconnection\n23.2.5 Transactional Semantics\n• Network partition: part of network isolated from rest\n– Allow conflicting operations on replicas across file partitions\n– Reconcile upon reconnection\n– Transactional semantics => operations must be serializable\n∗ Ensure that operations were serializable after thay have executed\n– Conflict => force manual reconciliation\n23.2.6 Client Caching\n• Cache consistency maintained using callbacks\n\nLecture 23: April 28 23-5\nFigure 23.5: An example of nodes in xFS\n23.3 xFS\n23.3.1 Overview of xFS\n• Key Idea: fully distributed file system [serverless file system]\n– Remove the bottleneck of a centralized system\n• xFS: x in ”xFS” = no server\n• Designed for high-speed LAN environments\n• All nodes participates in the File sharing\nxFS combines two main concepts ; RAID - (Redundant Array of Inexpensive Disks) and Log Structured\nFile Systems (LFS). It uses a concept of Network Stripping and RAID over a network wherein, a file is\npartitioned into blocks and provided to different servers. These blocks are then made as a Software RAID\nfile by computing a parity for each block which resides on a different machine.\n23.3.2 RAID : Redundant Array of Independent Disks\nIn RAID based storage, files are striped across multiple disks. Disk failures are to be handled explicitly in\ncase of a RAID based storage. Fault tolerance is built through redundancy.\nFigure 23.6: Striping in RAID\n\n23-6 Lecture 23: April 28\nFigure23.6showshowfilesarestoredinRAID.d1,...d4aredisks. Eachfileisdividedintoblocksandstored\nin the disks in a round robin fashion. So if a disk fails, all parts stored on that disk are lost.\nMTTF : Mean time to failure. It is about 5-6 years for a disk.\nA typical disk lasts for 50,000 hours which is also knows as the Disk MTTF. As we add disks to the system,\nthe MTTF drops as disk failures are independent.\nReliability of N disks=Reliability of 1 disk÷N (23.1)\nProbability of failure of a system=(1-p)n (23.2)\nConsider a case where there are 70 disks in the system.\nReliability of system = 50, 000 Hours ÷70disks=700hours\nDisk system MTTF: Drops from 6 years to 1 month!\n23.3.2.1 Advantages\n• Load balanced across multiple disks\n• Parallelizes the access to each disk and hence high throughput.\n23.3.2.2 Disadvantages\n• If a single disk fails, 1/N of the data of each file will be lost, without redundancy.\n• The performance of this system depends on the reliability of disks.\nWe implement some form of redundancy in the system to avoid disadvantages caused by disk failures.\nDepending on the type of redundancy the system can be classified into different groups:\n23.3.2.3 RAID 0\nDoesn’t have any redundancy. Only striping. Each files in stripped into multiple parts and stores on a\nseparate disk.\n23.3.2.4 RAID 1 (Mirroring)\nFrom figure 23.7, we can see that in RAID 1 each disk is fully duplicated. Each logical write involves two\nphysical writes. This scheme is not cost effective as it involves a 100% capacity overhead.\n\nLecture 23: April 28 23-7\nFigure 23.7: RAID 1\nFigure 23.8: RAID 4\n23.3.2.5 RAID 4\nThis method uses parity property to construct ECC (Error Correcting Codes) as shown in Figure 23.8. All\nparity blocks are stored on the same disk. First a parity block is constructed from the existing blocks.\nSupposetheblocksD , D , D andD arestripedacross4disks. Afifthblock(parityblock)isconstructed\n0 1 2 3\nas:\nP =D ⊕D ⊕D ⊕D (23.3)\n0 1 2 3\nIf any disk fails, then the corresponding block can be reconstructed using parity. For example:\nD =D ⊕D ⊕D ⊕P (23.4)\n0 1 2 3\nThis error correcting scheme is one fault tolerant. Only one disk failure can be handled using RAID 4. The\nsize of parity group should be tuned so as there is low chance of more than 1 disk failing in a single parity\ngroup.\nSmaller writes are expensive, as the corresponding parity would need to be changed and it would require\nreading the other members of the parity group. This involves N+2 operations; N block reads and 2 block\nwrites (Read-modify-write cycle).\nWrites seen by the parity disk is N times the writes seen by the other disks. Therefore, the parity disk will\nquickly get overloaded if you have a write-intensive system.\nQuestion: Where is the information about which files are on which disk?\nAnswer: The hardware controller serves the request internally to identify which blocks are stored on which\ndisk.\nQuestion: InRAID,hardwarecontrollerkeepsatrackofdatablocksandparity,whathappensifcontroller\nfails?\nAnswer: There will be problems in accessing the disk. That may be a point of failure. In case of Software\nRAID this issue will not occur.\n\n23-8 Lecture 23: April 28\nQuestion: Won’t the cost of accessing files increase since all disks are being accessed?\nAnswer: There are two ways to access a file, either block by block or accessing the whole file. If a request\nis made to access the whole file, in the above figure, eight requests to different disks would have been made,\nmaking it parallel. If the entire file would have been saved on the same disk, it would have resulted in eight\nrequests being made to the single disk, which makes it sequential. Thus, accessing multiple disks does not\nnecessarily make it expensive.\nQuestion:Can you read D0 and the parity,P and xor it to get the middle blocks?\nAnswer:This would be an a good optimization idea. But there is still additional overhead given there is\nstill a read-modify-write cycle.\n23.3.2.6 RAID 5\nFigure 23.9: RAID 5\nOne of the main drawbacks of RAID 4 is that all parity blocks are stored on the same disk. Also, there are\nk + 1 I/O operations on each small write, where k is size of the parity block. Moreover, load on the parity\ndiskissumofloadonotherdisksintheparityblock. Thiswillsaturatetheparitydiskandslowdownentire\nsystem.\nIn order to overcome this issue, RAID 5 uses distributed parity as shown in Figure 23.9. The parity blocks\nare distributed in an interleaved fashion.\nNote: All RAID solutions have some write performance impact. There is no read performance impact.\nRAIDimplementationsaremostlyonhardwarelevel. HardwareRAIDimplementationaremuchfasterthan\n\nLecture 23: April 28 23-9\nsoftware RAID implementations.\nQuestion: Can a file not be stored in the same disk as its parity?\nAnswer: The parity and the file stripe wouldn’t be in the same parity group. It can still handle 1 disk\nfailure.\n23.3.2.7 RAID Summary\n• Basic idea: files are ”striped” across multiple disks\n• Redundancy yields high data availability\n- Availability: service still provided to user, even if some components failed\n• Disks will still fail\n• Contents reconstructed from data redundantly stored in the array\n- Capacity penalty to store redundant info\n- Bandwidth penalty to update redundant info\n23.3.3 LFS: Log File Structure\nIn log structured File systems, data is sequentially written in the form of a log. The motivation for LFS\nwould be the large memory caches used by the OS. Larger, the size of cache, more the number of cache hits\ndue to reads, better will be the payoff due to the cache. The disk would be accessed only if there is a cache\nmiss. Due to the this locality of access, mostly write requests would trickle to the disk. Hence, the disk\ntraffic comes predominantly from write. In traditional hard drive disks, a disk head read or writes data .\nHence, to read a block, a seeks needs to be done i.e. move the head to the right track on the disk.\nHow to optimize a file system which sees mostly write traffic ?\nThe basic insight is to reduce the time spent on seek and waiting for the required block to spin by. Every\nread/write request incurs a seek time and a rotational latency overhead. In general , random access layout\nis assumed for all blocks in the disk wherein the next block is present in an arbitrary location. This would\nrequire a seek time.\nTo eliminate this, a sequential form of writing facilitated by LFS can be used. The main idea of LFS is\nthat we try to write all the blocks sequentially one after the other. Thus LFS essentially buffers the writes\nand writes them in contiguous blocks into segments in a log like fashion. This will dramatically improve the\nperformance. Anynewmodificationwouldbeappendedattheendofthecurrentlogandhence, overwriting\nis not allowed. Any LFS requires a garbage collection mechanism to de-fragment and clean holes in the log.\nHence, XFS ensures 1. fault tolerance - due to RAID, 2. Parallelism - due to blocks being sent to multiple\nnodes. 3. High Performance - due to Log structured organization.\nIn SSD’s, the above mentioned optimization to log structures doesn’t give any benefits since there are no\nmoving parts and hence, no seek.\nQuestion: Is there an overhead to maintain lookup as block of the files need to be tracked?\nAnswer: There is higher overhead to maintain the lookup. For every write, the data gets appended, so it\n\n23-10 Lecture 23: April 28\nis meant to be for high write workloads. Metadata of the files is also written to the log. In case of lookups,\nthe metadata has to be accessed. Hence there is high overhead.\nQuestion: Can the writes be cached?\nAnswer: Reads are directly cached. Writes are cached in batches i.e. a batch of writes are written as an\nappend-only log.\nQuestion: Is LFS one server?\nAnswer: LFS are traditionally designed as single disk system. Here, they are combined with xFS. The logs\nare stripped across machines.\n23.3.3.1 Log-structured FS Summary\n• Provide fast writes, simple recovery, flexible file location method\n• Key Idea: buffer writes in memory and commit to disk in large, contiguous, fixed-size log segments\n– Complicates reads, since data can be anywhere\n– Use per-file inodes that move to the end of the log to handle reads\n– Uses in-memory imap to track mobile inodes\n∗ Periodically checkpoints imap to disk\n∗ Enables ”roll forward” failure recovery\n– Drawback: must clean ”holes” created by new writes\n23.3.4 xFS Summary\n• Distributes data storage across disks using software RAID and log-based network striping.\n• Dynamically distribute control processing across all servers on a per-file granularity\n- Utilizes serverless management scheme.\n• Eliminates central server caching using cooperative caching\n- Harvest portions of client memory as a large, global file cache.\n23.3.5 xFS uses software RAID and LFS\nxFS uses software RAID because the disks are not on one machine. Striping occurs across a network and\nuses software RAID to update the parity.\n• Two limitations\n– Overhead of parity management hurts performance for small writes\n∗ Ok, if overwriting all N-1 data blocks\n∗ Otherwise, must read old parity+data blocks to calculate new parity\n∗ Small writes are common in UNIX-like systems\n– Very expensive since hardware RAIDS add special hardware to compute parity\n\nLecture 23: April 28 23-11\n23.3.6 Combine LFS with Software RAID\nLogwrittensequentiallyarechoppedintoblockswhichaparitygroups. Eachparitygroupbecomesaserver\non a different machine in a RAID fashion\nFigure 23.10: Combining LFS with Software RAID\nQuestion: In RAID 5, if one disk has failed, how do you know where the parts of the file are stored?\nAnswer: RAID is operating below the file system. It doesn’t know anything about files, it only knows\nabout blocks. The file system is the one that knows the metadata that keeps track where the data blocks\nare stored.\n23.4 HDFS - Hadoop Distributed File system\n• It is designed for high throughput - very large datasets. Optimized for read only applications.\n• HDFS is designed with specific goals:\n– Fault Tolerant: It has to have fault tolerance built in.\n– Streaming data access: It is designed for batch processing rather than interactive processing.\n– Large datasets: scale to hundreds of nodes.\n– Simple coherency model: HDFS has a simple coherency model in which it assumes a WORM\n(Write Once Read Many) model. In WORM, file do not change and changes are append-only.\n– Move computation to the data when possible.\n23.4.1 Architecture\nThere are 2 separate kinds of nodes in HDFS ; Data and Meta-data nodes. Data nodes store the data\nwhereas, meta-data keeps track of where the data is stored. Average block size in a file system is 4 KB.\nIn HDFS, due to large datasets, block size is 64 MB. Replication of data prevents disk failures. Default\nreplication factor in HDFS is 3.\n\n23-12 Lecture 23: April 28\nFigure 23.11: HDFS Architecture\nFigure 23.12: GFS Architecture\n23.5 GFS - Google File System\nMasternode actsas ameta-data server. Ituses afile systemtree tolocate thechunks (GFSterminology for\nblocks). Each chunk is replicated on 3 nodes. Each chunk is stored as a file in Linux file system.\n23.6 Object Storage Systems\n• Use handles(e.g., HTTP) rather than files names\n– Location transparent and location independence\n– Separation of data from metadata (similar to HDFS and GFS)\n• No block storage: objects of varying sizes\n• Uses\n- Archival storage\n– can use internal data de-duplication\n- Cloud Storage: Amazon S3 service\n\nLecture 23: April 28 23-13\n– uses HTTP to put and get objects and delete\n– Bucket: objects belong to bucket/partitions name space\nQuestion What does ”Location transparent and location independence” imply?\nAnswer: ThinkofahandleasalargenumericID,lookingatthehandleyouwillnotknowwheretheobject\nis stored. This explains the location transparency. For the location independence, Internally the system can\ndecide to move the object somewhere else and the handle name remains the same.\nIn Amazon’s case, they replicate the buckets across data centers. They assume entire data centers can fail.\nThey keep the objects replicated geographically and the object handle will not tell you the location and\ninternally will figure out the closest available location and get that data for the user."
  },
  "Lec22_slides": {
    "url": "https://lass.cs.umass.edu/~shenoy/courses/677content/slides/spring25/Lec22.pdf",
    "parsed_text": "Distributed File Systems\n• Overview of UNIX file systems\n• Issues in distributed file systems\n• NFS\n• Next time: case studies of distributed file systems\n• Coda\n• xFS\n• Log-structured file systems (time permitting)\n• HDFS; object storage systems\nCompsci 677: Distributed and OS Lec. 22 1\nFile System Basics\n• File: named collection of logically related data\n• Unix file: an uninterpreted sequence of bytes\n• File system:\n• Provides a logical view of data and storage functions\n• User-friendly interface\n• Provides facility to create, modify, organize, and delete files\n• Provides sharing among users in a controlled manner\n• Provides protection\nCompsci 677: Distributed and OS Lec. 22 2\n\nUnix File System Review\n• User file: linear array of bytes. No records, no file types\n• Directory: special file not directly writable by user\n• File structure: directed acyclic graph [directories may not be shared, files may be shared (why?) ]\n• Directory entry for each file\n• File name\n• inode number\n• Major device number\n• Minor device number\n• All inodes are stored at a special location on disk [super block]\n• Inodes store file attributes and a multi-level index that has a list of disk block locations for the file\nCompsci 677: Distributed and OS Lec. 22 3\nInode Structure\n• Fields\n• Mode\n• Owner_ID, group_id\n• Dir_file\n• Protection bits\n• Last access time, last write time, last inode time\n• Size, no of blocks\n• Ref_cnt\n• Address[0], … address[14]\n• Multi-level index: 12 direct blocks, one single, double, and triple indirect blocks\nCompsci 677: Distributed and OS Lec. 22 4\n\nDistributed File Systems\n• File service: specification of what the file system offers\n• Client primitives, application programming interface (API)\n• File server: process that implements file service\n• Can have several servers on one machine (UNIX, DOS,…)\n• Components of interest\n• File service\n• Directory service\nCompsci 677: Distributed and OS Lec. 22 5\nFile Service\n• Remote access model\n•Upload/download mode\n– Work done at the server\n– Work done at the client\n• Stateful server (e.g., databases) •Stateless server\n• Consistent sharing (+) •Simple functionality (+)\n• Server may be a bottleneck (-) •Moves files/blocks, need storage (-)\n• Need for communication (-)\nCompsci 677: Distributed and OS Lec. 22 6\n\nSystem Structure: Server Type\n• Stateless server\n• No information is kept at server between client requests\n• All information needed to service a requests must be provided by the client with each request (what info?)\n• More tolerant to server crashes\n• Stateful server\n• Server maintains information about client accesses\n• Shorted request messages\n• Better performance\n• Idempotency easier\n• Consistency is easier to achieve\nCompsci 677: Distributed and OS Lec. 22 7\nNFS Architecture\n• Sun’s Network File System (NFS) – widely used distributed file system\n• Uses the virtual file system layer to handle local and remote files\nCompsci 677: Distributed and OS Lec. 22 8\n\nNFS Operations\nOperation v3 v4 Description\nCreate Yes No Create a regular file\nCreate No Yes Create a nonregular file\nLink Yes Yes Create a hard link to a file\nSymlink Yes No Create a symbolic link to a file\nMkdir Yes No Create a subdirectory in a given directory\nMknod Yes No Create a special file\nRename Yes Yes Change the name of a file\nRmdir Yes No Remove an empty subdirectory from a directory\nOpen No Yes Open a file\nClose No Yes Close a file\nLookup Yes Yes Look up a file by means of a file name\nReaddir Yes Yes Read the entries in a directory\nReadlink Yes Yes Read the path name stored in a symbolic link\nGetattr Yes Yes Read the attribute values for a file\nSetattr Yes Yes Set one or more attribute values for a file\nRead Yes Yes Read the data contained in a file\nWrite Yes Yes Write data to a file\nCompsci 677: Distributed and OS Lec. 22 9\nCommunication\na) Reading data from a file in NFS version 3.\nb) Reading data using a compound procedure in version 4.\nBoth versions use Open Network Computing (ONC) RPCs\n- One RPC per operation (NFS v3); multiple operations supported in v4.\nCompsci 677: Distributed and OS Lec. 22 10\n\nNaming: Mount Protocol\n• NFS uses the mount protocol to access remote files\n• Mount protocol establishes a local name for remote files\n• Users access remote files using local names; OS takes care of the mapping\nCompsci 677: Distributed and OS Lec. 22 11\nNaming: Crossing Mount Points\n• Mounting nested directories from multiple servers\n• NFS v3 does not support transitive exports (for security reasons)\n• NFS v4 allows clients to detects crossing of mount points, supports recursive lookups\nCompsci 677: Distributed and OS Lec. 22 12\n\nAutomounting\n• Automounting: mount on demand\nCompsci 677: Distributed and OS Lec. 22 13\nFile Attributes (1)\nAttribute Description\nTYPE The type of the file (regular, directory, symbolic link)\nSIZE The length of the file in bytes\nCHANGE Indicator for a client to see if and/or when the file has changed\nFSID Server-unique identifier of the file's file system\n• Some general mandatory file attributes in NFS.\n• NFS modeled based on Unix-like file systems\n• Implementing NFS on other file systems (Windows) difficult\n• NFS v4 enhances compatibility by using mandatory and recommended attributes\nCompsci 677: Distributed and OS Lec. 22 14\n\nFile Attributes (2)\nAttribute Description\nACL an access control list associated with the file\nFILEHANDLE The server-provided file handle of this file\nFILEID A file-system unique identifier for this file\nFS_LOCATIONS Locations in the network where this file system may be found\nOWNER The character-string name of the file's owner\nTIME_ACCESS Time when the file data were last accessed\nTIME_MODIFY Time when the file data were last modified\nTIME_CREATE Time when the file was created\n• Some general recommended file attributes.\nCompsci 677: Distributed and OS Lec. 22 15\nSemantics of File Sharing\na) On a single processor, when a read follows a\nwrite, the value returned by the read is the\nvalue just written.\nb) In a distributed system with caching, obsolete\nvalues may be returned.\nCompsci 677: Distributed and OS Lec. 22 16\n\nSemantics of File Sharing\nMethod Comment\nUNIX semantics Every operation on a file is instantly visible to all processes\nSession semantics No changes are visible to other processes until the file is closed\nImmutable files No updates are possible; simplifies sharing and replication\nTransaction All changes occur atomically\n• Four ways of dealing with the shared files in a distributed system.\n• NFS implements session semantics\n• Can use remote/access model for providing UNIX semantics (expensive)\n• Most implementations use local caches for performance and provide session semantics\nCompsci 677: Distributed and OS Lec. 22 17\nFile Locking in NFS\nOperation Description\nLock Creates a lock for a range of bytes (non-blocking_\nLockt Test whether a conflicting lock has been granted\nLocku Remove a lock from a range of bytes\nRenew Renew the lease on a specified lock\n• NFS supports file locking\n• Applications can use locks to ensure consistency\n• Locking was not part of NFS until version 3\n• NFS v4 supports locking as part of the protocol (see above table)\nCompsci 677: Distributed and OS Lec. 22 18\n\nFile Locking and Share Reservations\nCurrent file denial state\nNONE READ WRITE BOTH\nRequest\nREAD Succeed Fail Succeed Fail\naccess\nWRITE Succeed Succeed Fail Fail\nBOTH Succeed Fail Fail Fail\n(a)\nRequested file denial state\nCurrent NONE READ WRITE BOTH\naccess READ Succeed Fail Succeed Fail\nstate\nWRITE Succeed Succeed Fail Fail\nBOTH Succeed Fail Fail Fail\n(b)\n• The result of an open operation with share reservations in NFS.\na) When the client requests shared access given the current denial state.\nb) When the client requests a denial state given the current file access state.\nCompsci 677: Distributed and OS Lec. 22 19\nClient Caching\n• Client-side caching is left to the implementation (NFS does not prohibit it)\n• Different implementation use different caching policies\n• Sun: allow cache data to be stale for up to 30 seconds\nCompsci 677: Distributed and OS Lec. 22 20\n\nClient Caching: Delegation\n• NFS V4 supports open delegation\n• Server delegates local open and close requests to the NFS client\n• Uses a callback mechanism to recall file delegation.\nCompsci 677: Distributed and OS Lec. 22 21\nRPC Failures\n• Three situations for handling retransmissions: use a duplicate request cache\na) The request is still in progress\nb) The reply has just been returned\nc) The reply has been some time ago, but was lost.\nUse a duplicate-request cache: transaction Ids on RPCs, results cached\nCompsci 677: Distributed and OS Lec. 22 22\n\nSecurity\n• The NFS security architecture.\n• Simplest case: user ID, group ID authentication only\nCompsci 677: Distributed and OS Lec. 22 23\nSecure RPCs\n• Secure RPC in NFS version 4.\nCompsci 677: Distributed and OS Lec. 22 24\n\nReplica Servers\n• NFS ver 4 supports replications\n• Entire file systems must be replicated\n• FS_LOCATION attribute for each file\n• Replicated servers: implementation specific\nCompsci 677: Distributed and OS Lec. 22 25"
  },
  "Lec22_notes": {
    "url": "https://lass.cs.umass.edu/~shenoy/courses/677content/notes/spring25/Lec22_notes.pdf",
    "parsed_text": "CMPSCI 677 Distributed Operating Systems Spring 2025\nLecture 22: April 23\nLecturer: Prashant Shenoy Scribe: Srikrushna Pinaki Budi (2025), Shrutiya Mohan (2024)\n22.1 File System Basics\n22.1.1 File\nA file is a container of data in text format, binary format etc. which is stored on a disk so that the user can\nre-visit it at a later point in time. In UNIX, a file is an uninterpreted sequence of bytes which implies that\nthe file system is unaware of the contents/type of the file. Other operating systems like Windows and Mac\nknows the file types (This information can be useful to open a file in the right application).\n22.1.2 File System\n• File system abstracts and provides a logical view of data (a hierarchy of files and folders) and storage\nfunctions.\n• It helps us to create, modify, organize and delete files and takes care of how to map them to the\nunderlying storage device.\n• Itprovidesauser-friendlyinterfacesothattheuserneednotdealwiththelow-levelinterfacesexported\nby the disk.\n• It allows us to share the files among other users by giving permissions and also allows us to protect\nthe files.\n22.1.3 UNIX File System Review\n• In UNIX, the files structure can be viewed as a directed acyclic graph. Note that this looks like a tree\nstructure but can contain soft links pointing from one directory to other which makes it a DAG. Each\ndirectory entry for each file contains the file name, inode number (metadata for the file), major device\nnumber and minor device number. All inodes are stored at a particular location on the disk called\nsuper block. To access the file, the file system needs to first get this metadata to know where the file\nis located in the actual disk (aka block locations of the file).\n• An inode structure consists of the fields like mode, Owner ID, group id, Dir file, protection bits, last\naccess time, size, reference count, address[0]...address[14] etc. The addresses stores the pointers to the\ndata blocks. The first 12 are the direct blocks which stores the pointers to the data blocks (see figure\n22.1),the13thaddressstoresthepointerstothelocationwhichinturnstoresthepointerstothedirect\ndata blocks (one level of indirection). The 14th address follows two levels of indirection which stores\nthe pointers to one level indirection blocks. So the hierarchy grows as the size of the file grows but we\nhave an upper limit of the size of the file that can be stored on this file system because we only have\na certain number of pointers in addresses (In this case from 0 till 14).\n22-1\n\n22-2 Lecture 22: April 23\nFigure 22.1: Inode structure\n22.2 Distributed File Systems (DFS)\nIf files on a different machine can be accessed, it is a distributed file system. Another way to think about\nDFS is that the servers store different files on different servers, and all the servers collectively form your file\nsystem.\n22.2.1 File server\nA machine that stores all the files.\n22.2.2 File service\nThe interface that the machine exposes for other machines to access the files on this machine. For example\nNFS uses RPCs to send read/write requests to a remote file system. There are two types of file services as\nshown in figure 22.2.\n• Remote access model: The client requests are sent to the server and the server sends back the results\nafter doing the work requested by the client. This model is typically stateful since we need to keep\ntrack of which clients are accessing which file and so on. This might eventually cause the server to\nbecome a bottleneck if there are many incoming requests from multiple clients (I/O bottleneck at the\nserver).\n• Upload/downloadmodel: Whenthe clientperforms arequest tothe server, entire file is sentas acopy\nto the client, and subsequent access are made to the local copy. To maintain consistency, the client\neventually sends back the changed file to the server. This model works only if there is one client one\nfile at a time, hence maintaining consistency.\n\nLecture 22: April 23 22-3\nNote: As the files are directly updated on the server, there is consistency in the remote access model, but\neach operation is an RPC call which makes it slow. In upload/download model, there is a period of time in\nwhichthefileontheserverisoutofdate. Havingsaidthat,upload/downloadmodelgivesbetterperformance\nas the operations are taking place on the local machine and very less calls are made to the remote machine.\nFigure 22.2: Remote access model(left), Upload/download model(right).\nQuestion: Which model does Google Docs use?\nAnswer: Google docs is a cloud service and not a distributed file system in a technical sense. Today,\nGoogle, One drive, Dropbox provide a form of remote storage that looks like distributed file system, but is\nnot necessarily the same. Answering the question, google docs model depends on the mode of the browser.\nIn the online version, every change is saved on the cloud server instantly, while in the offline mode, there is\na copy at the client and a master copy at the server.\n22.2.3 Server Type\nThere are two types of server and one would need to make the choice from one of them when building any\ndistributed file system.\nStateless server: No information about clients is kept at the server.\nStateful server: Server maintains information about the client accesses. It is less tolerant to failures because\nthe state is lost when a server crashes. There is slight performance benefit here due to the compact request\nmessages(Clientsdonotneedtosendtheinformationlikepermissionseverytimetherequestisbeingmade).\nConsistency and idempotency are easier to achieve.\nNote: An Idempotent server executes as if it has performed the request only once regardless of how many\ntimes same request was received.\n22.2.4 Network File System (NFS)\nNFS is a layer on top of an existing file system that allows to share the file system over a network. NFS is\nimplementedusingvirtualfilesystemlayersupportedbytheunderlyingoperatingsystem. Virtualfilesystem\nlayercanbeseenasaforwardinglayerthatlooksatwhereisthefilestoredandinvokethatfilesystem(local\nfile system for local files and possibly NFS for remote files). Here, the client and server communicate with\neachotehrusingRPCcalls. TheVFSlayerintheclientandserverprovidesasystemindependentabstraction\nto the layer above it. Thus, no need to worry about the type of the filesystem the file is stored on.\nNote: Till version 3, NFS used stateless server protocol but from version 4, it uses stateful server protocol.\nSo it now supports open call to a remote file.\n\n22-4 Lecture 22: April 23\nFigure 22.3: Network File System (NFS)\nFigure 22.4: Difference in communication in NFS version 3 and version4.\nIn figure 22.4, we can observe that in version 3 of NFS, individual LOOKUP and READ RPC calls were\nneeded whereas in version 4 of NFS, we can perform a batch RPC request. Version 3 executes one RPC per\noperation, whereas version 4 supports multiple RPC calls per operation(batched).\nQuestion: Is the NFS Client and Server implemented as a user space process or is it implemented inside the\nkernel?\nAnswer: It is an in kernel implementation. It is not a user space process. In most operating systems, NFS\ncode is going to reside inside the kernel like any other file system code but this is just distributed in nature.\nQuestion: Why is Lookup triggered?\nAnswer: We usually access a file by opening the file and reading it. This eventually will go to the OS as\na system call that’s called an open system call at the OS level. This will go to a virtual file system layer\nand then the NFS Client and client has to service the open but version3 does not have a concept of an open\nso instead it is going to send a lookup operation to the server saying client wants to access this file. Then\nchecks if file name is valid and if client has privileges to access it. It sends a response in a yes or no ( yes if\nopen call succeeds) then we get another read system call and another RPC is triggered.\n22.2.5 Mount protocol\nMount protocol is a way how a NFS client gets access to a remote file system. Certain directories can be\nmapped from remote file system to the local file system in order to get access.\n\nLecture 22: April 23 22-5\nFigure 22.5: Mount Protocol\nQuestion: Client A can access few files in the mounted directory and Client B can access few files in the\nmounted directory, can you see all of those files?\nAnswer: Thevisibilityofthesefilesiscontrolledbyfilepermissions. SimilartoUnixwesetthefilepermissions\nwhich mention read write and execution permissions to other users. OS will control this and file sharing can\nbe done in distributed systems in the same way.\nQuestion: Does mounting create a list of files stored on the server or are you always going to lookup files on\nthe server?\nAnswer: Mounting is simply a mapping. It is not going to create any list. Mounting creates a mapping\nfrom the directory searched to the directory’s location on the server. Client OS is not going to know what\nis stored in the directory. So when the user tries to access anything inside the directory, all the operations\nare sent to the server and whatever responses are returned are checked.\nQuestion: Is concurrent access possible in this model?\nAnswer: There are two types of concurrent accesses: accessing same file, accessing same volume of files.\nIt is of course possible, example Ed Lab with multiple users accessing the file volume on the system and\nworking on it concurrently. To access the same file, it is possible but we need locking mechanisms to avoid\noverwriting each other’s data.\n22.2.6 Crossing mount points\nCrossing mount points is mounting nested directories from multiple servers. NFS v3 does not support\ntransitive exports NFS v4 allows clients to detect crossing of mount points and supports recursive lookups.\nQuestion: What happens if the Client tries to access inner nested file which is local to Server B?\nAnswer: In NFS version 3 it does not allow transitive exports for security reason. It will return an error.\nBut in version 4 first RPC request will go to Server A which will then forward it to Server B and perform\nthe operation and send back the response.\n22.2.7 Automounting\nAutomounting is also known as mounting on demand. The mappings get established but the mounting only\nhappens when the user tries to access those directories. And if there is an idle time, it unmounts. This way\nwe can reduce the amount of kernel resources used.\n\n22-6 Lecture 22: April 23\nFigure 22.6: Crossing Mount Points\nFigure 22.7: Automounting\n22.2.8 File attributes\nThere are specific attributes (like TYPE, SIZE, CHANGE, FSID) that a file system must to support to be\ncompatible with NFS. There are other attributes (like OWNER of a file) which are not mandatory to be\ncompatible with NFS but are recommended.\n22.2.9 Semantics of file sharing\n• In UNIX semantics, every operation on a file is instantly visible to all the other processes using the\nsame file.\n• In session semantics, no changes are visible to other processes until the file is closed.\n• Immutable files cannot be mutated. A new version of the file needs to be created if we need any\nchanges.\n• In transaction semantics, all changes occur atomically.\nNote: NFS follows semantics in between UNIX and session. It caches the file and periodically flushes the\n\nLecture 22: April 23 22-7\nchanges to the server. If one process writes to a file, the other process might have a different or outdated\nversion of the file for a period of time. NFS uses local caches for performance reasons which leads to this\nweak consistency.\n22.2.10 File locking in NFS\n• Version3ofNFSusedstatelessserverprotocol. Oneoftheusesofhavingastateisfilelocking. Version\n4 of NFS uses stateful server protocol, so applications can now use locks to ensure consistency. File\nlocking can be done in different ways like locking the entire file, locking a specific range of bytes in a\nfile etc.\n• In share reservations file locking, we have the notion of a denial state where if an application has a\nwrite denial state to a file, it cannot write to the file but it can read the file.\n22.2.11 Client Caching: Delegation\nFigure 22.8: Delegation\nNFS supports the concept of delegation as part of caching. The client receives a master copy of the file to\nwhich the client can make updates. Upon completion, the client can send the file back to the server. This\nis similar to the concept of upload/download model. Thus, the server is delegating the file to the client so\nthattheclientcanhavealocalcopy. Ifanotherclienttriestoaccessthefile,theserverrecallsthedelegation\ngiven to previous client. The previous client returns the file to the server and then the server uses the old\nmodel where multiple clients can access the file by read/write requests to the sever.\nQuestion: When does the server decide to delegate the file?\nAnswer: Since this feature is stateful, it is only present in version 4. If the server is serving only one client\nthen the server can delegate the file. Otherwise since the server is not the current owner of the file, the\nserver can not delegate and thus has to use the old model. For example, files in the user’s home directory\ncan be delegated, whereas binaries of application programs can not be delegated as multiple users might\naccess them.\nQuestion: Is there a way to periodically update the server as in case of client failure the files may get lost?\nAnswer: It is possible for the client to flush the changes to the server in the background while it still holds\nthe master copy.\nQuestion: Theclientisupdatingthefileandtheserverrecallsthedelegation,whathappenstotheupdate?\nAnswer: It is fine for the client to perform all outstanding operations, have them finish and then send the\nfile back. There is no need to cancel any write operations, it can wait and then send the latest version and\n\n22-8 Lecture 22: April 23\nall subsequent accesses of the file back to the server, as we cannot have any local copy on the client. The\nopen call will just take longer because the file has to come back to the server.\nQuestion: Does it have any security concern? Can the file have make any unsanctioned updates?\nAnswer: Theunsanctioned updates arenotanissue becauseiftheprocess hasreadandwrite accesstothe\nfile, it can write whatever it wants. Access Control is the responsibility of the OS.\n22.2.12 RPC Failures\nFigure 22.9: RPC Failures\nFor RPCs being used over TCP, TCP will take care of retransmissions between client and server. For RPCs\nover UDP, client and server can decide how to deal with lost requests and replies. Every RPC request is\nassociated with an ID. Upon receiving an RPC request from the client, the server will maintain the request\nand response for that request in its cache. If the client resends the request and the reply was lost, the server\nwill simply send the reply from the cache, instead of executing it again.\nQuestion: What is the utility of UDP over TCP?\nAnswer: UDP is faster than TCP as there is three-way handshake in TCP. In LANs, where probability of\nloss is low, RPCs can be sent over UDP. Over WANs or noisy LANs TCP may be preferred.\nQuestion: For how long can the reply be kept in the cache?\nAnswer: It depends on the application. Practically, after some unsuccessful tries within an hour, the client\nmay assume that the server is down. So the replies can be cached for some hours.\nQuestion: Is caching reply specific to some version of NFS?\nAnswer: It is not specific to some verison of NFS. In NFS v1, there was no concept of RPCs over TCP.\nThus,thismethodwasusedforRPCsoverUDP.Currently,withtheadventofRPCsoverTCP,thismethod\nis not needed to be used.\nQuestion: Is the cache needed only so that the requests are idempotent?\nAnswer: Yes. Forexample,ifrequestsarechangingfiles,itmightincurproblemsiftheyarenotidempotent\nand if requests are needed to be idempotent the cache is required.\nQuestion: Who is responsible for keeping the IDs unique, client or the server?\nAnswer: The IDs have to be generated at the client. This can be made unique by using the client’s IP\naddress followed by a number that is incremented sequentially for each RPC call.\n\nLecture 22: April 23 22-9\nFigure 22.10: Secure RPCs\n22.2.13 Security\nVersions 1, 2 and 3 of NFS relied on a simple security model. Every request is sent with user ID and\ngroup ID. The server checks for the file permissions on the basis of user ID and server ID. This ensures only\nauthenticated users can access the file. One drawback of this is that the channel between client and server,\nhowever, is not still secure. If an adversary intercepts the network traffic, the contents of a secure file may\nbe exposed. In version 4, the concept of secure RPCs was introduced. Every RPC client stub sends the\nrequest to the security layer which encrypts the request before sending. Thus, file contents can not be read\non the network.\nQuestion: Client can send user ID and group ID, but how does the server know if it is authentic?\nAnswer: As long as the server trusts the OS on the client the server knows the user ID and group ID are\nauthentic. However, if the OS is corrupted/hacked the server can not trust the client\n22.2.14 Replica Servers\nThere may be multiple servers serving different set of files. Version 4 allows the files to be replicated. Client\ncan make request for accessing files from any of the replicas. NFS provides implementation of maintaining\nconsistency between the replicated servers."
  },
  "Lec21_slides": {
    "url": "https://lass.cs.umass.edu/~shenoy/courses/677content/slides/spring25/Lec21.pdf",
    "parsed_text": "Web Caching\n• Example of the web to illustrate caching and replication issues\n– Simpler model: clients are read-only, only server updates data\nrequest\nWeb\nbrowser\nserver\nresponse\nrequest request\nWeb Proxy Web\nbrowser\ncache server\nresponse response\nCompsci 677: Distributed and OS Lec. 21 1\nWeb Proxy Caching\n• The principle of cooperative caching.\nCompsci 677: Distributed and OS Lec. 21 2\n\nConsistency Issues\n• Web pages tend to be updated over time\n– Some objects are static, others are dynamic\n– Different update frequencies (few minutes to few weeks)\n• How can a proxy cache maintain consistency of cached data?\n– Send invalidate or update\n– Push versus pull\nCompsci 677: Distributed and OS Lec. 21 3\nPush-based Approach\n• Server tracks all proxies that have requested objects\n• If a web page is modified, notify each proxy\n• Notification types\n–Indicate object has changed [invalidate]\n–Send new version of object [update]\npush\n• How to decide between invalidate and updates?\nWeb\nproxy\n–Pros and cons? server\n–One approach: send updates for more frequent objects, invalidate for rest\nCompsci 677: Distributed and OS Lec. 21 4\n\nPush-based Approaches\n• Advantages\n–Provide tight consistency [minimal stale data]\n–Proxies can be passive\n• Disadvantages\n–Need to maintain state at the server\n• Recall that HTTP is stateless\n• Need mechanisms beyond HTTP\n–State may need to be maintained indefinitely\n• Not resilient to server crashes\nCompsci 677: Distributed and OS Lec. 21 5\nPull-based Approaches\npoll\nWeb\nproxy\nserver\nresponse\n• Proxy is entirely responsible for maintaining consistency\n• Proxy periodically polls the server to see if object has changed\n–Use if-modified-since HTTP messages\n• Key question: when should a proxy poll?\n–Server-assigned Time-to-Live (TTL) values\n• No guarantee if the object will change in the interim\nCompsci 677: Distributed and OS Lec. 21 6\n\nPull-based Approach: Intelligent Polling\n• Proxy can dynamically determine the refresh interval\n– Compute based on past observations\n• Start with a conservative refresh interval\n• Increase interval if object has not changed between two successive polls\n• Decrease interval if object is updated between two polls\n• Adaptive: No prior knowledge of object characteristics needed\nCompsci 677: Distributed and OS Lec. 21 7\nPull-based Approach\n• Advantages\n–Implementation using HTTP (If-modified-Since - conditional GET)\n–Server remains stateless\n–Resilient to both server and proxy failures\n• Disadvantages\n–Weaker consistency guarantees (objects can change between two polls and proxy will contain stale data\nuntil next poll)\n• Strong consistency only if poll before every HTTP response\n–More sophisticated proxies required\n–High message overhead\nCompsci 677: Distributed and OS Lec. 21 8\n\nA Hybrid Approach: Leases\n• Lease: duration of time for which server agrees to notify proxy of modification\n• Issue lease on first request, send notification until expiry\n– Need to renew lease upon expiry\n• Smooth tradeoff between state and messages exchanged\n– Zero duration => polling, Infinite leases => server-push\n• Efficiency depends on the lease duration\nGet + lease req\nread\nClient Proxy\nReply + lease Server\nInvalidate/update\nCompsci 677: Distributed and OS Lec. 21 9\nPolicies for Leases Duration\n• Age-based lease\n– Based on bi-modal nature of object lifetimes\n– Larger the expected lifetime longer the lease\n• Renewal-frequency based\n– Based on skewed popularity\n– Proxy at which objects is popular gets longer lease\n• Server load based\n– Based on adaptively controlling the state space\n– Shorter leases during heavy load\nCompsci 677: Distributed and OS Lec. 21 10\n\nCooperative Caching\n• Caching infrastructure can have multiple web proxies\n– Proxies can be arranged in a hierarchy or other structures\n• Overlay network of proxies: content distribution network\n– Proxies can cooperate with one another\n• Answer client requests\n• Propagate server notifications\nCompsci 677: Distributed and OS Lec. 21 11\nHierarchical Proxy Caching\nServer\nHTTP\n3\nParent ICP HTTP\nICP 2 ICP\nLeaf Caches\n1\nHTTP Read A\nClients\nExamples: Squid, Harvest\nCompsci 677: Distributed and Operating Systems Lec. 22 12\n\nLocating and Accessing Data\nServer\nfor B\nGet A Get B\nNode Y Node X\nNode Z\nRead A\n(A,X) Caches\nRead B\nClients\nMinimize cache hops on hit Do not slow down misses\nProperties\n• Lookup is local\n• Hit at most 2 hops\n• Miss at most 2 hops (1 extra on wrong hint)\nCompsci 677: Distributed and Operating Systems Lec. 22 13\nEdge Computing\n• Web caches effective when deployed close to clients\n• At the “Edge” of the network\n• Edge Computing: paradigm where applications run on servers located at the edge of the network\n• Benefits\n• Significantly lower latency than “remote” cloud servers\n• Higher bandwidth\n• Can tolerate network or cloud failures\n• Complements cloud computing\n• Cloud providers offer edge servers as well as cloud servers\nCompsci 677: Distributed and OS Lec. 21 14\n\nEdge Computing Origins\n• Origins come from mobile computing and web caching\n• Content delivery networks\n• Network of edge caches deployed as commercial service\n• Cache web content (especially rich content: images, video)\n• Deliver from closest edge proxy server\n• Mobile computing\n• devices has limited resources, limited battery power\n• computational offload: offload work to more capable edge server\n• low latency offload important for interactive mobile applications\n• edge server sends results to the mobile\nCompsci 677: Distributed and OS Lec. 21 15\nContent Delivery Networks\n• Global network of edge proxies to deliver web content\n• edge clusters of varying sizes deployed in all parts of the world\n• Akamai CDN: 120K servers in 1100 networks, 80 countries\n• Content providers are customers of CDN service\n• Examples: news sites, image-rich online stores, streaming sites\n• Decide what content to cache/offload to CDN\n• Embed links to cdn content: http://cdn.com/company/foo.mp4\n• Consistency responsibility of content providers\n• Users access website normally\n• Some content fetched by browser from CDN cache\nCompsci 677: Distributed and OS Lec. 21 16\n\nCDN Request Routing\n• Web request need to be directed to nearby CDN server\n• Two level load balancing\n• Global: decide which cluster to use to serve request\n• Local: decide which server in the cluster to use\n• DNS-based approach is common\n• Special DNS server: resolve www.cnn.com/newsvideo.mp4\n• DNS checks location of client and resolves to IP address of nearby CDN server\n• Different users will get resolved to different edge locations\nCompsci 677: Distributed and OS Lec. 21 17\nCDN Issues\n• Which proxy answers a client request?\n–Ideally the “closest” proxy\n–Akamai uses a DNS-based approach\n• Propagating notifications\n–Can use multicast or application level multicast to reduce overheads (in push-based approaches)\n• Active area of research\n–Numerous research papers available\nCompsci 677: Distributed and OS Lec. 21 18\n\nCDN Request Processing\n• The principal working of the Akamai CDN.\nCompsci 677: Distributed and OS Lec. 21 19\nCDN Hosting of Web Applications\n• Figure 12-21. Alternatives for caching and replication with Web applications.\nCompsci 677: Distributed and Operating Systems Lec. 22 20\n\nMobile Edge Computing\n• Use case: Mobile offload of compute-intensive tasks\n• Example: augmented reality, virtual reality (mobile AR/VR)\n• mobile phone or headset has limited resources, limited battery\n• Low latency / response times for interactive use experience\n• mobile devices may lack a GPU or mobile GPU may be limited\n• Today’s smartphones are highly capable (multiple cores, mobile GPU, neural processor)\n• mobile offload more suitable for less capable devices (e.g., AR headset)\n• 5G cellular providers: deploy edge servers near cell towers\n• industrial automation, autonomous vehicles\nCompsci 677: Distributed and OS Lec. 21 21\nEdge Computing Today\n• Emerging approach for latency-sensitive applications\n• Edge AI: run AI (deep learning) inference at edge\n• home security camera sends feed, run object detection\n• Low latency offload: autonomous vehicles, smart city sensors, smart home etc.\n• Edge computing as an extension to cloud\n• Cloud regions augmented by local regions\n• Local regions are edge clusters that offer normal cloud service (but at lower latency) E.g.,\nAWS Boston region\n• Internet of Things (IoT) data processing sevices\nCompsci 677: Distributed and OS Lec. 21 22\n\nSpecialized Edge Computing\n• Edge accelerators: special hardware to accelerate edge tasks on resource\nconstrained edge servers\n• Nvidia Jetson GPU, Google edge Tensor processing Unit (TUP), Intel\nVision Processing Unit (VPU)\n• Example: IoT ML inference on edge accelerators\n• Efficient inference on resource-constrained edge servers\nApple Neural\nGoogle Edge TPU\nNvidia Jetson Nano GPU\nEngine\nCompsci 677: Distributed and Operating Systems Lec. 22 23\nCloud and Edge Architectures\n•\nOffload to cloud, edge, specialized edge,\ncloud cloud server\n+ GPU/FPGA\ncloud\nedge nodes\nEdge node\n+ VPU/TPU\nIoT device\nIoT device\nIoT device with accelerator\nTraditional cloud\nTraditional edge Specialized\n(2-tier)\n(3-tier) (3-tier)\nCompsci 677: Distributed and Operating Systems Lec. 22 24"
  },
  "Lec21_notes": {
    "url": "https://lass.cs.umass.edu/~shenoy/courses/677content/notes/spring25/Lec21_notes.pdf",
    "parsed_text": "CMPSCI 677 Distributed Operating Systems Spring 2025\nLecture 21: April 18\nLecturer: Prashant Shenoy Scribe: Sergei Pogorelov (2025) Ibrahim Hasaan (2024)\n21.1 Web Caching\nFigure 21.1: Client-Server and Client-Proxy-Server Architectures\nWeb caching uses a client-proxy-server architecture. Clients send requests to the proxy. Proxies can service\nthe requests directly if they have the resources. If they do not have the resources, they go to the server to\nget the request processed. In web caching, the proxy provides the service of caching i.e. the proxy caches\ncontentfromtheserverandwhentheclientbrowsersmakearequest,ifthecontentisalreadyinthecache,it\nreturns the content. This helps in faster responses for the client, if the proxy is near the client, and reduced\nloads for the server.\nNote: Web pages can either be dynamic or static. Dynamic web pages are regenerated for every request and\nfor every user, as opposed to static web pages which can just be pregenerated HTML files which only change\nwhentheuserchangesthem. Therefore, dynamicwebpagesaretypicallynotcached. Forthisreason, content\nbeing referred to in these notes is largely static content (static audio, video, web pages etc).\n21.2 Web Proxy Caching\nThe discussion for this section assumes a collection of proxy caches sitting in between the client and the\nserver. Along with communicating with the server, these proxy caches can also communicate with each\nother. This mechanism is called “Cooperative Caching”.\nFigure 21.2 shows one such scenario where a client sends a request to the web proxy. The web proxy will\nthenlookintoitslocalcachefortherequestedwebpage. Ifitisahit,thenitwillsendtheresponseback. In\nthe case of miss, typically the web proxy will contact server. But in the case of cooperative caching, cache\nmisses can be serviced by asking a nearby/local proxy instead of the server i.e. it will reach out to the near\nby proxies to get the data. In this case, all the caches act like one big logical cache, so the clients will see\nunion of all the content stored in nearby caches rather than just the content cached in the local proxy. This\ncan make fetching faster than getting data from the server.\n21-1\n\n21-2 Lecture 21: April 18\nFigure 21.2: Web Proxy Caching\n21.3 Consistency Issues\nRecall a system has consistency if all the replicas see the same data at the same time. In the context of web\ncaching, this means that when a browser fetches a page, we are guaranteed that the returned page is the\nmost recent version. When designing consistency techniques for web caching, it’s important to keep in mind\nthat web pages can vary significantly in terms of how frequently they are updated and accessed:\n• Some objects are static, meaning once created, they never change.\n• Other web pages are dynamic and change frequently (e.g., news websites with breaking news sections\nthat update every few minutes).\n• Update frequency can range from a few minutes to a few weeks to never.\n• Access frequency (popularity) also varies, with some pages receiving millions of users per day, while\nothers get very little access.\nThere are 2 approaches for maintaining consistency - pull-based and push-based.\n21.3.1 Push based Approach\nThis approach relies on the server, i.e. it is the responsibility of the server to ensure that the content stored\nat the proxy caches is up to date. For every web page stored at the server, the server keeps a table of all\nproxies that have a cached copy of that web page. Whenever a page is updated, the server looks at the\ntable,findsalltheproxiesthathaveacopyofthewebpageandnotifieseachofthemthatthepagehasbeen\nupdated.\nThis notification can be of 2 types:\n1. Invalidate - inform the proxy that the web page has changed (so it can discard the page from its\ncache).\n2. Update - send the new version of the page. This tells the proxy that the page was changed and that\nit should replace the old version (in its cache) with the new one.\n\nLecture 21: April 18 21-3\nQuestion: When to use invalidate and when to use the update message?\nAnswer: This depends on the read and update frequency of the content.\n• Invalidate: Use invalidate for less popular pages (low access frequency). Since fewer clients will\nrequest the updated page, this avoids wasting bandwidth sending updates to proxies unnecessarily.\n• Update: Use updates for frequently accessed pages, as most proxies will request the updated page\nafterreceivingtheinvalidatemessage. Sendingtheupdatedcontentdirectlyavoidsredundantrequests.\nAdvantages:\n• Provides relatively tight consistency guarantees.\n• Proxies can remain passive; the server manages consistency.\nDisadvantages:\n• Theserverneedstomaintainsubstantialstate,includinginformationaboutwhichproxieshavecached\nspecific content.\n• Requires the server to remain stateful, which increases complexity.\n• Failure recovery becomes more difficult due to the state being lost when the server crashes.\nQuestion: Given that the cache size at a proxy is limited, what happens when old items are removed due\nto cache replacement? How does the server know that content has been evicted from the proxy’s cache?\nAnswer: When the cache size is limited, proxies must implement a cache replacement policy, such as\nLeast Recently Used (LRU), where new items are brought in and old or less frequently used items are\nremoved.\nThe server, however, needs to track whether cached content is still present in the proxy to avoid sending\nunnecessary updates or invalidation messages. To inform the server about evictions:\n1. The proxy could notify the server whenever an item is removed from its cache.\n2. If such notifications are not implemented, the server will not be aware of specific evictions and might\nsend wasteful update or invalidation messages unnecessarily.\nThis additional communication adds overhead to the system but ensures the server has an accurate under-\nstanding of the proxy’s cache state.\nQuestion: While the invalidate message is in transit, the cache is already stale. How to deal with this?\nAnswer: We cannot deal with this issue because even if the server instantly sends the message there is still\na speed of light propagation delay for the message to reach the proxy. For this period of time, the proxy is\ngoing to have stale code. Strict consistency in practice is hard to achieve.\n21.3.2 Pull based Approach\nIn a pull-based approach, the proxy is responsible for maintaining cache consistency by periodically\nquerying the server to check whether the cached page is still valid. This is accomplished using HTTP\n\n21-4 Lecture 21: April 18\nconditional GET messages (e.g., “If-modified-since”). Server will return modified web page only if it has\nchanged.\nWhen to poll for web pages?\nDependsonthefrequencyofupdates. Forwebpageswhichchangeveryrarely, frequentpollingisextremely\nwasteful. There are no such wasteful messages in the push based approach. If the page changes much more\nfrequently than the polling frequency, then the proxy might cache outdated content for longer times. Thus,\npolling frequency should be decided based on the update frequency of web page. There are two ways to do\nthis:\n1. Server can assign an expiration time, TTL(time-to-live). This time is an estimation of next possible\nchangesonthewebpage. Servercanestimateitbasedonthepastwebpageupdatefrequencyhistory.\nIt is likely that web page might change after TTL so poll after TTL expires.\n2. Proxyhasintelligencetodynamicallyfigureoutthepollingtimes. Polldurationisvariedbasedonthe\nobserved web page updates. Dynamically change polling frequency to understand the average rate at\nwhich web page is changing.\nAdapting polling frequency dynamically:\n• Initially poll frequently to detect how often the page changes.\n• Over time, adjust polling to match observed update frequency (increase intervals for static objects,\npoll more frequently for dynamic objects).\nQuestion: In a pull-based approach, do we have distinct message types like in the push-based approach,\nwhere there are separate notify messages for invalidation and updates? Or is it handled differently?\nAnswer: In the pull-based approach, it works a bit differently. You are always using a conditional GET,\nwhich functions as both a notify and an update:\n• If the content has not changed, the conditional GET returns a null, essentially serving as a notify\nmessage, informing the proxy that nothing has changed.\n• If the content has changed, the condition in the GET request evaluates to true. In this case, the\nupdated content is fetched, making it equivalent to an update.\nThus, instead of separate message types as in the push-based approach, the conditional GET in the pull-\nbased approach combines both functionalities. Its response behaves like a notify or an update depending on\nwhether the condition is true or false.\nQuestion: Why not just poll when you get the request?\nAnswer: We could do this, this is the only way we get strong consistency because when a request comes\nand it’s a cache hit. But we don’t know if it’s consistent, so we can do an If-modified-since request to check\nif the cache still consistent.\nQuestion: What is the downside of the approach mentioned in the above question?\nAnswer: Increase in latency. The reason we are caching the content at the proxy is that proxies are close\nto the client and they can deliver content faster. However, before delivering that content every time if you\nhave to go to the server to check, it introduces another round trip time delay. You may as well go to the\nserver and fetch the content directly.\nQuestion: In the pull-based approach, does the server have additional overhead because of the additional\nIf-modified-since http messages?\n\nLecture 21: April 18 21-5\nAnswer: Yes,thereisadditionaloverheadbecausethesechecksmayneedtobemadefrequentlytomaintain\nconsistency.\nQuestion: Does DNS use a pull-based approach?\nAnswer: DNS is a pull-based protocol in the sense that it translates hostnames to IP addresses, and the\nbrowser sends it a hostname for which the DNS returns the IP address. DNS also uses caching, for which it\njust uses TTL values (since IP addresses don’t change that frequently).\nQuestion: Why can’t we do a hybrid of both approaches (Pull-based and Push-based)? Why can’t the\ndeveloper then decide, or the proxy decide dynamically?\nAnswer: The server has to support a push-based approach. For example, if the server was just a standard\nhttp server then it won’t have any push-based functionality. If the server does, then you could do a hybrid\napproach.\nQuestion: Do we need to worry about clock synchronization?\nAnswer: Not too much, assume clocks use NTP accuracy of 10 milliseconds. As web pages won’t change\nevery 10 milliseconds (usually changed in days or weeks), we need not worry about synchronization.\nQuestion: Polling is to keep the content in the cache consistent with the server, but how does the proxy\nknow whether to keep the cached content at all? Maybe nobody is interested in it.\nAnswer: Tounderstandwhenaproxyshouldmaintaincacheandwhennottoithastotracksomestatistics\n(like the rate a page is requested at). If the request rate is very low, the proxy can decide the page is no\nlonger popular and evict it. On the other hand, if the request rate for a page is high proxy will decide to\nkeep it consistent.\nQuestion: Pull-based approach does not have persistent HTTP connections, is that a limitation of this\napproach? And does server push have persistent connection?\nAnswer: Neither approach requires persistent connections. If polling frequency is less does not make sense\nto keep persistent connections - wasting lot of resources. Only makes sense to do this when content is\nchanging frequently.\nAdvantages and Disadvantages\nPull based approach gives weaker consistency guarantees. Updates of a web page at server, might not be\nimmediately reflected at proxies. Latency to synchronize the content might overtake the benefits of proxies.\nThereisahigheroverheadthantheserverpushapproach,andtherecouldbemorepullsthanupdates. Some\nadvantages are that the pull based approach can be implemented using HTTP (server remains stateless).\nThis approach is also resilient to both server and proxy failures.\n21.3.3 A Hybrid Approach: Leases\nHybrid approach based on both push and pull. Figure 21.3 illustrates how it works.\nFigure 21.3: Lease\nLease is a contract between two entities (here the server and the proxy). It specifies the duration of time\n\n21-6 Lecture 21: April 18\nthe server agrees to notify the proxy about any updates on the web page. Updates are no longer sent by the\nserver after lease expiration, and the server will delete the state. Proxy can renew the lease. If the page is\nunpopular, proxy can decide not to renew the lease for that page.\nLease is a more limited form of server push - performing push for the duration of the lease only. We get\nadvantages of push and do not have to keep state indefinitely. If the lease duration is zero, it degenerates to\npolling (pull). If duration is infinite, it is the same as push-based and makes the server stateful. Duration\nin between zero and infinite is a combination of both pull and push.\nTuning the lease duration:\n• Long leases: Suitable for frequently accessed content or objects that change infrequently.\n• Shortleases: Suitableforrarely accessedcontentorwhentheserverishighlyloaded, asthisreduces\nthe cost of tracking state at the server.\nTight consistency guarantees when lease is active.\n21.3.4 Policies for Leases Duration\nLease duration is an important parameter. There are three policies for lease duration:\n1. Age-based: Based on frequency of changes to the object. The age is the time since last update.\nAssign longer leases for more frequently updated pages.\n2. Renewal-frequency based: Based on frequency of access requests from clients. Popular objects get\nlonger leases.\n3. Server load based: If the load on the server is high, we should use shorter lease duration. This will\nremove the burden of storing proxy state on server side.\nQuestion: What is age-based lease?\nAnswer: First, what does age mean here? The age of a file or web content is the time since it was modified\nlast i.e. time now - time file was last modified. If a file has old age it means it hasn’t been modified and a\nyounger file means they’ve been modified recently. We can use age to decide how long a lease should be.\nQuestion: If the file is modified more frequently, the age will be lower so the lease time will be longer?\nAnswer: Intheslideitsaystogivelongerleasestotheobjectswithlargerlifetimes,butyoucandoexactly\nthe opposite of this depends if you want to reduce the workload on the server or not. Giving long leases for\nfrequently updated objects increases the server load.\nQuestion: If a proxy evicts an object from the lease, can the lease be canceled? Answer: Original lease\nmechanism does not have any cancellation mechanism, but you can add one. Ideally, if you have an active\nlease you should not evict an object but if you do for any reason, there has to be either a way to cancel the\nlease or you’ll get some wasted notifications from the server.\nQuestion: Does each proxy have its own lease?\nAnswer: Not only does each proxy have a lease, there is a lease for each web page at a proxy. If a proxy\ncaches 100 different web pages, each of them will have a different lease. It is not a lease per proxy, it is a\nlease per web page at a proxy. Different proxies will have different leases for the same page.\nQuestion: Is it the proxy’s responsibility to keep track of the lease or the server’s responsibility?\nAnswer: Both. Server has to keep track of all active leases and send notifications whenever the web page\n\nLecture 21: April 18 21-7\nchanges for every active lease on that web page. Proxy has to track lease as well. If the lease expires, proxy\ndecides whether to renew it or not.\nQuestion: Do you need a separate monitoring framework to keep track of popularity of content?\nAnswer: Server by itself will not know how popular the content is. Server can know the age and server\nload. Proxies can track popularity of the web page and report stats to the server. Monitoring framework\ncan be added to the proxy system, it is not a part of the lease approach.\n21.3.5 Cooperative Caching\nRecall, in cooperative caching a collection of proxies cooperate with each other to service client requests.\nThese proxies can be arranged in different structures. In “Hierarchical Proxy Caching”, the proxies are\narranged in a hierarchy, a tree-like structure, where the server is the root and the rest of the nodes are\ncaches.\nFigure 21.4 shows an example of this. The client sends a request to one of the proxies. If it is a cache\nmiss, the proxy will send requests to its peers (red arrows) and parent using ICP (Internet Cache Protocol)\nmessages. If none of the peers have it, they will send back the non-availability response to the proxy (green\narrows). The proxy will then forward the HTTP request to its parent and the whole process will recurse\nuntil a cache hit occurs or until the server is reached. The data will then be sent back as response - and will\nflow down the hierarchy, back to the client.\nQuestion: Why are we using ICP instead of HTTP?\nAnswer: ICP is designed specifically for caching and cache consistency, whereas HTTP is not. ICP is\nbasically just a way to ask other nodes if they have some content and fetch the content if they do, so its not\nvery different to HTTP in that sense.\nQuestion: What if the proxies have different versions of the file?\nAnswer: Here, the assumption is that if any one proxy has the content, then that content can be sent. To\nmaintain consistency, whenever a proxy knows that the content has changed it can inform other proxies of\nthe latest version.\nFigure 21.4: Hierarchical Proxy Caching\nThis works well when a nearby proxy actually has the content. If there is a global miss - no one in the\nhierarchy has the content, latency will increase significantly. Clearly, there is a lot of messaging overhead.\nAlso,browserhastowaitforlongertimesinthecaseofcachemissontheproxy. Thiswillaffectperformance.\nLatency could increase - it may have been faster to just directly request from the server in the event of a\n\n21-8 Lecture 21: April 18\ncache miss on the proxy.\nTo address this problem, we’ll look at a different approach for doing this. Every time we send a request up\nthe chain, it adds more hops which increases the overhead and thus the latency, so firstly we’ll remove the\nhierarchy. This gives us a flat structure where every proxy directly communicates with all other proxies. If\nthe content for a request is in a nearby cache, it is fetched. Otherwise, the content is fetched directly from\nthe server.\nMoreover, we want to reduce the overhead of querying other proxies to see if they have a page or not. To\nachieve this, every proxy keeps track of what is stored in its nearby caches. Now, whenever a request comes\nin, iftheproxydoesn’thavethedatainitscache, itlooksinthetabletoseeifanynearbycachehasit. And\nlikewise, if there is no entry in the table for a request i.e. none of the nearby caches have the content, it gets\nit from the server and adds it to the table.\nUsing this approach, lookup is local. A hit is at most 2 hops and a miss is also at most 2 hops (as opposed\nto 1 hop in the other method). Every time the proxy fetches or deletes a page, it updates the table for all\nthe nodes. Every proxy keeps a global table that must be kept consistent. There is an additional overhead\nfor keeping this consistent, but the performance is better.\nFigure 21.5: Locating and Accessing Data in the Flattened Network\nQuestion: Do we force the miss by enforcing that it goes to at most two hops?\nAnswer: Yes, two is the most we will have because there is no re-forwarding. If a request comes in from\nanotherproxy,wewillnotcheckifsomeotherproxyhasit. So,arequestfromaclientandonefromanother\nproxy are treated differently.\nQuestion: Is this approach going to be more efficient than the previous one? Because we have to inform\nall other caches after every cache update.\nAnswer: There are two things to keep in mind. Firstly, the caches are not going to change that frequently.\nSecondly,thepurposeofhavingproxiesisthatthelatencytimesforclientsarereduced. Updatescanhappen\nin the background, so they don’t add to clients’ latency.\nQuestion: Why could we not use Hierarchical Proxy Caching if there are only popular web pages?\nAnswer: We cannot assume users will only request for popular pages they can ask for any page.\nQuestion: In the server push approach or lease approach, can you use multicast to notify all proxies?\nAnswer: Thenotificationscanbesentaseithernunicastmessages,onetoeachproxythathasthecontent,\nor a single multicast message, where all proxies are listening, so that is a more efficient way of sending\nmessages. That is independent of whether a lease is being used or not.\nFollow-up Question: If you do multicast, don’t you require a lease?\nAnswer: The reason a lease is required is if you don’t have a list of which proxies to notify and send an\nupdate to every proxy in the system, maybe only 10 out of 10000 proxies have the content. You have now\n\nLecture 21: April 18 21-9\nwasted messages by sending message to 10000 proxies. Even though it is a multicast message, you are still\nusing network resources to send it. If you want to multicast, you want to send it only to the proxy group\nthat has the content and so you need to track that.\nQuestion: If you want to do multicast, how do you identify proxies that have the content?\nAnswer: You would have to construct a multicast group for every web page. When a proxy caches that\ncontent, you put that proxy in that group. When the content is removed from the proxy, you remove the\nproxy from the group.\nQuestion: If you have a hierarchical proxy caching system, can different proxies use different consistency\nmechanisms - some push some pull?\nAnswer: Thatwouldbeaproblem. Sincecachesareinteractingwitheachother,itisbettertouseuniform\nconsistency mechanism.\n21.4 Edge Computing\nItistheevolutionofproxyserversintoamoregeneralapproachwhereserversaredeployedattheedgeofthe\nnetworkandtheyprovideaservice. Theseserverscanprovidemorethanjustcachingservices. Applications\ncan be run on these servers. Edge computing is a paradigm where applications run on servers located at the\nedge of the network. Benefits include lower network latency than remote cloud servers, higher bandwidth\nand can tolerate network or cloud failures.\nCloud computing platforms are treating edge computing as an extension of cloud computing where cloud\nresources are being deployed closer to users rather than in distant data centers.\n21.4.1 Edge Computing Origins\nEdge computing evolved simultaneously from mobile computing and web caching.\nContentDeliveryNetworks-Aswebcachingbecamepopular,severalcommercialprovidersofferedproxy\ncaching as a service - they deployed proxy caches in many different networks. If you were an operator of a\nwebapp,youcouldbecomeacustomerandtheycouldcacheyourcontentanddelivertoyourcustomers,for\na small fee. These companies deployed CDNs - large network of proxy caches deployed all over the world.\nYou could offload your content to these proxy caches and deliver to end users at low latency. This network\nof caches was an early form edge computing.\nMobile Computing - Early mobile devices were resource and energy constrained. Not advisable to do\nheavycomputationsonthesedevices. Oneapproachwastoputserversneartheedgeofthewirelessnetwork.\nComputationally intensive tasks were offloaded to the edge server. This was called computation offloading -\noffloading work from one device (mobile device) to another (edge server). This approach was also an early\nform of edge computing by offloading computation at low latency.\n21.4.2 Content Delivery Networks (CDNs)\nGlobal network of edge proxies that provide caching services among other services to deliver web content.\nUseful to deliver rich content like images or video content which can increase the load on server significantly\nas its better to cache such content to reduce load on server. Commercial CDNs deploy many these servers\nin many different networks. Servers are deployed as clusters of different sizes depending on the demand.\n\n21-10 Lecture 21: April 18\nContent providers are customers of the CDN service. They decide what content to cache and it is their\nresponsibility to maintain consistency.\nUsers access website normally, the content is fetched by the browser from CDN cache.\n21.4.2.1 CDN Request Processing\nFigure21.6showshowarequestisroutedfromaclienttoaCDNserver. Theclientisgoingtogotheserver\nand make a request for an html page (this is not cached, so it comes from the origin server). Within the\npage, there may be content; the URL for that content will point to one of the CDN servers.\nNow, the CDN has to decide which of its caches will give the content. This is done through a smart DNS\nlookup. Recall DNS (”Domain Name Service”) is a service which takes the url and gives the IP address.\nThe browser makes a connection to this returned IP address.\nIn a smart DNS lookup, the DNS Server will check where the client is located and return the IP address of\nthe nearest cache.\nFigure 21.6: CDN Request Processing\n21.4.2.2 CDN Request Routing\nWhenaCDNgetsarequest, itneedstosendtothenearestcache(togetthelowestlatency), sohowdoesit\ndecidewhichproxyhastoservethatrequest? Theyhavelargeloadbalancersthatlookatincomingrequests\nand send it to different proxies. Typical CDNs have 2 level load balancer:\n1. Global level - Which cluster to send the request to. This is done using DNS (as described above).\n2. Locallevel-Oncetherequestismappedtoacluster,whichserverintheclusterwillservetherequest?\n(When a request is routed to a cache, it will usually not be a single cache, but a cluster of caches).\nQuestion: If it’s a local load balancer does it use concepts like least loaded, round robin?\nAnswer: Yes, that is exactly what the local load balancer does. As long as content is replicated, you send\nit to any of the caches; if its notyou have to look at the urland only send it to the subset of them that have\nit.\n\nLecture 21: April 18 21-11\nQuestion: Do edge proxies use cooperative caching?\nAnswer: In this case, no need to do cooperative caching. Essentially replicating content and local load\nbalancing will ensure that the server you are getting mapped to has a copy of the content.\nQuestion: Are DNS and CDN independent services?\nAnswer: In this case, the DNS is run by the CDN server. Your browser will send a request to your local\nDNS. That DNS server will send that request to another server responsible for the domain you are going\nafter. For example cnn.com/newsvideo.mp4. Cnn would, in this case, have the CDN run its DNS service\nfor it. So, the request goes to CDN where all of this happens i.e deciding the closest server. DNS is indeed\nseparatefromCDNsbutsomeDNSserversinthiscasearegoingtoberunbyCDNandthosearetheservers\nfor domains that it is actually caching content for.\nQuestion: Is there criteria apart from geographical proximity that is used to do load balancing?\nAnswer: This is the case. Proximity is not the only criterion, there will be a lot more sophistication to\nhandle overload etc. For example - fault tolerance has to be built in.\nCDNs have evolved from simple caches to running entire applications at the edge. Figure 21.7 shows CDN\nhosting web apps. Dynamic content is not cacheable so caches are less useful for CDNs. So CDNs allow\nrunning applications on edge server at low latency.\nFigure 21.7: CDN Hosting web apps\n21.4.3 Mobile Edge Computing\nAllows mobile devices to offload compute-intensive tasks to edge servers. Use cases are mobile AR/VR\nwhere the mobile device had to process graphics heavy content that drained battery life faster and heavy\nduty computation was required. Since users are interacting with the system, low latency is very important.\nEdge servers provided both compute power as well as low latency.\nMobile devices today are much more capable and need to offload from smartphones has reduced. Other\ndevicestodaythatarenotascapablesuchasheadsetsstillusemobileedgecomputingforoffloadingcompute\nintensive tasks."
  },
  "Lec20_slides": {
    "url": "https://lass.cs.umass.edu/~shenoy/courses/677content/slides/spring25/Lec20.pdf",
    "parsed_text": "Distributed Web Applications\n• WWW principles\n• Case Study: web caching as an illustrative example\n– Invalidate versus updates\n– Push versus Pull\n– Cooperation between replicas\nCompsci 677: Distributed and OS Lec. 20 1\nTraditional Web-Based Systems\n• Client-server web applications\nCompsci 677: Distributed and OS Lec. 20 2\n\nWeb Browser Clients\n• The logical components of a Web browser.\nCompsci 677: Distributed and OS Lec. 20 3\nThe Apache Web Server\n• The general organization of the Apache Web server.\nCompsci 677: Distributed and OS Lec. 20 4\n\nProxy Servers\n• Using a Web proxy when the browser does not speak FTP (or for caching and\noffloading)\nCompsci 677: Distributed and OS Lec. 20 5\nMultitiered Architectures\n• Three tiers: HTTP, application, and database tier\nCompsci 677: Distributed and OS Lec. 20 6\n\nWeb Server Clusters\n• Clients connect to front-end dispatcher, which forwards requests to a replica (recall\ndiscussion from Cluster scheduling)\n• Each replica can be a tiered system\n– For consistency, database can be a common/non-replicated\nCompsci 677: Distributed and OS Lec. 20 7\nWeb Server Clusters (2)\n• A scalable content-aware cluster of Web servers.\nCompsci 677: Distributed and OS Lec. 20 8\n\nWeb Clusters\n• Request-based scheduling\n• Forward each request to a replica based on a policy\n• Session-based scheduling\n• Forward each session to a replica based on a policy\n• Scheduling policy: round-robin, least loaded\n• HTTP redirect vs TCP splicing vs TCP handoff\nCompsci 677: Distributed and OS Lec. 20 9\nElastic Scaling\n• Web workloads: temporal time of day, seasonal variations\n• Flash crowds: black friday, sports events, news events\n• Overloads can occur even with clustering and replication\n• Elastic scaling: dynamically vary capacity based on workload (aka auto-scaling, dynamic provisioning)\n• Two approaches:\n• Horizontal scaling: increase or decrease # of replicas based on load\n• Vertical scaling: increase or decrease size of replica (e.g., # of cores allocated to container or VM) based on load\n• Proactive versus reactive scaling\n• Proactive: predict future load and scale in advance\n• Reactive: scale based on observed workload\n• Common in large cloud-based web applications\nCompsci 677: Distributed and OS Lec. 20 10\n\nMicro-services Architecture\n• Micro-services: application is a collection of smaller services\n• Example of service-oriented architecture\n• Modular approach to overcome “monolith hell”\n• Each microservice is small and can be maintained independently of others\n• Each is independently deployable\n• Clustering and auto-scaling can be performed independently\nCompsci 677: Distributed and OS Lec. 20 11\nScaling Web applications\n• Three approaches for scaling\nhttps://microservices.io/articles/scalecube.html\nCompsci 677: Distributed and OS Lec. 20 12\n\nWeb Documents\n• Six top-level MIME types and some common subtypes.\nCompsci 677: Distributed and OS Lec. 20 13\nHTTP Connections\n• Using nonpersistent connections.\nCompsci 677: Distributed and OS Lec. 20 14\n\nHTTP 1.1 Connections\n• (b) Using persistent connections.\nCompsci 677: Distributed and OS Lec. 20 15\nHTTP Methods\n• Operations supported by HTTP.\nCompsci 677: Distributed and OS Lec. 20 16\n\nHTTP 2.0\n• Http 1.1 allows pipelining over same connection\n• Most browsers do not use this feature\n• HTTP v2: Designed to reduce message latency\n• No new message or response types\n• Key features\n• Binary headers (over text headers of http 1.1)\n• Uses compression of headers and messages\n• Multiplex concurrent connections over same TCP connection\n• each connection has multiple “streams”, each carrying a request and response\n• No blocking caused by pipelining in http 1.1\nSee https://developers.google.com/web/fundamentals/performance/http2/\nCompsci 677: Distributed and OS Lec. 20 17\nWeb Services Fundamentals\n• The principle of a Web service.\nCompsci 677: Distributed and OS Lec. 20 18\n\nSimple Object Access Protocol\n• An example of an XML-based SOAP message.\nCompsci 677: Distributed and OS Lec. 20 19\nRESTful Web Services\n• SOAP heavy-weight protocol for web-based distributed computing\n• RESTful web service: lightweight , point-to-point XML comm\n• REST=representative state transfer\n• HTTP GET => read\n• HTTP POST => create, update, delete\n• HTTP PUT => create, update\n• HTTP DELETE => delete\n• Simpler than RPC-sytle SOAP\n• closer to the web\nCompsci 677: Distributed and OS Lec. 20 20\n\nRESTful Example\nHTTP/1.1 200 OK\nContent-Type: text/xml; charset=utf-8\nContent-Length: nnn\nGET /StockPrice/IBM HTTP/1.1\nHost: example.org\n<?xml version=\"1.0\"?>\nAccept: text/xml\n<s:Quote xmlns:s=\"http://example.org/stock-service\">\nAccept-Charset: utf-8\n<s:TickerSymbol>IBM</s:TickerSymbol>\n<s:StockPrice>45.25</s:StockPrice>\n</s:Quote>\nCompsci 677: Distributed and OS Lec. 20 21\nCorresponding SOAP Call\nGET /StockPrice HTTP/1.1\nHost: example.org\nContent-Type: application/soap+xml; charset=utf-8\nContent-Length: nnn\n<?xml version=\"1.0a?>\n<env:Envelope xmlns:env=\"http://www.w3.org/2003/05/soap-envelope\"\nxmlns:s=\"http://www.example.org/stock-service\">\n<env:Body>\n<s:GetStockQuote>\n<s:TickerSymbol>IBM</s:TickerSymbol>\n</s:GetStockQuote>\n</env:Body> HTTP/1.1 200 OK\n</env:Envelope> Content-Type: application/soap+xml; charset=utf-8\nContent-Length: nnn\n<?xml version=\"1.0\"?>\n<env:Envelope xmlns:env=\"http://www.w3.org/2003/05/soap-envelope\"\nxmlns:s=\"http://www.example.org/stock-service\">\n<env:Body>\n<s:GetStockQuoteResponse>\n<s:StockPrice>45.25</s:StockPrice>\n</s:GetStockQuoteResponse>\n</env:Body>\n</env:Envelope>\nCompsci 677: Distributed and OS Lec. 20 22\n\nSOAP vs RESTful WS\n• Language, platform and • Language and platform\ntransport agnostic agnostic\n• Supports general distributed • Point-to-point only; no\ncomputing intermediaries\n• Standards based (WSDL, • Lack of standards support for\nUDDI dir. service...) security, reliability (“roll you\nown”\n• Builtin error handling\n• Simpler, less learning curve,\n• Extensible\nless reliance on tools\n• More heavy-weight\n• Tied to HTTP transport layer\n• Harder to develop • More concise\nCompsci 677: Distributed and OS Lec. 20 23\nWeb Proxy Caching\n• The principle of cooperative caching.\nCompsci 677: Distributed and OS Lec. 20 24\n\nWeb Caching\n• Example of the web to illustrate caching and replication issues\n– Simpler model: clients are read-only, only server updates data\nrequest\nWeb\nbrowser\nserver\nresponse\nrequest request\nWeb Proxy Web\nbrowser\ncache server\nresponse response\nCompsci 677: Distributed and OS Lec. 20 25\nConsistency Issues\n• Web pages tend to be updated over time\n– Some objects are static, others are dynamic\n– Different update frequencies (few minutes to few weeks)\n• How can a proxy cache maintain consistency of cached data?\n– Send invalidate or update\n– Push versus pull\nCompsci 677: Distributed and OS Lec. 20 26\n\nPush-based Approach\n• Server tracks all proxies that have requested objects\n• If a web page is modified, notify each proxy\n• Notification types\n–Indicate object has changed [invalidate]\n–Send new version of object [update]\npush\n• How to decide between invalidate and updates?\nWeb\nproxy\n–Pros and cons? server\n–One approach: send updates for more frequent objects, invalidate for rest\nCompsci 677: Distributed and OS Lec. 20 27\nPush-based Approaches\n• Advantages\n–Provide tight consistency [minimal stale data]\n–Proxies can be passive\n• Disadvantages\n–Need to maintain state at the server\n• Recall that HTTP is stateless\n• Need mechanisms beyond HTTP\n–State may need to be maintained indefinitely\n• Not resilient to server crashes\nCompsci 677: Distributed and OS Lec. 20 28\n\nPull-based Approaches\npoll\nWeb\nproxy\nserver\nresponse\n• Proxy is entirely responsible for maintaining consistency\n• Proxy periodically polls the server to see if object has changed\n– Use if-modified-since HTTP messages\n• Key question: when should a proxy poll?\n– Server-assigned Time-to-Live (TTL) values\n• No guarantee if the object will change in the interim\nCompsci 677: Distributed and OS Lec. 20 29\nPull-based Approach: Intelligent Polling\n• Proxy can dynamically determine the refresh interval\n– Compute based on past observations\n• Start with a conservative refresh interval\n• Increase interval if object has not changed between two successive polls\n• Decrease interval if object is updated between two polls\n• Adaptive: No prior knowledge of object characteristics needed\nCompsci 677: Distributed and OS Lec. 20 30\n\nPull-based Approach\n• Advantages\n–Implementation using HTTP (If-modified-Since)\n–Server remains stateless\n–Resilient to both server and proxy failures\n• Disadvantages\n–Weaker consistency guarantees (objects can change between two polls and proxy will contain stale data\nuntil next poll)\n• Strong consistency only if poll before every HTTP response\n–More sophisticated proxies required\n–High message overhead\nCompsci 677: Distributed and OS Lec. 20 31\nA Hybrid Approach: Leases\n• Lease: duration of time for which server agrees to notify proxy of modification\n• Issue lease on first request, send notification until expiry\n– Need to renew lease upon expiry\n• Smooth tradeoff between state and messages exchanged\n– Zero duration => polling, Infinite leases => server-push\n• Efficiency depends on the lease duration\nGet + lease req\nread\nClient Proxy\nReply + lease Server\nInvalidate/update\nCompsci 677: Distributed and OS Lec. 20 32\n\nPolicies for Leases Duration\n• Age-based lease\n– Based on bi-modal nature of object lifetimes\n– Larger the expected lifetime longer the lease\n• Renewal-frequency based\n– Based on skewed popularity\n– Proxy at which objects is popular gets longer lease\n• Server load based\n– Based on adaptively controlling the state space\n– Shorter leases during heavy load\nCompsci 677: Distributed and OS Lec. 20 33\nCooperative Caching\n• Caching infrastructure can have multiple web proxies\n– Proxies can be arranged in a hierarchy or other structures\n• Overlay network of proxies: content distribution network\n– Proxies can cooperate with one another\n• Answer client requests\n• Propagate server notifications\nCompsci 677: Distributed and OS Lec. 20 34\n\nHierarchical Proxy Caching\nServer\nHTTP\n3\nParent ICP HTTP\nICP 2 ICP\nLeaf Caches\n1\nHTTP Read A\nClients\nExamples: Squid, Harvest\nCompsci 677: Distributed and OS Lec. 20 35\nLocating and Accessing Data\nServer\nfor B\nGet A Get B\nNode Y Node X\nNode Z\nRead A\n(A,X) Caches\nRead B\nClients\nMinimize cache hops on hit Do not slow down misses\nProperties\n• Lookup is local\n• Hit at most 2 hops\n• Miss at most 2 hops (1 extra on wrong hint)\nCompsci 677: Distributed and OS Lec. 20 36\n\nEdge Computing\n• Web caches effective when deployed close to clients\n• At the “Edge” of the network\n• Edge Computing: paradigm where applications run on servers located at the edge of the network\n• Benefits\n• Significantly lower latency than “remote” cloud servers\n• Higher bandwidth\n• Can tolerate network or cloud failures\n• Complements cloud computing\n• Cloud providers offer edge servers as well as cloud servers\nCompsci 677: Distributed and OS Lec. 20 37\nEdge Computing Origins\n• Origins come from mobile computing and web caching\n• Content delivery networks\n• Network of edge caches deployed as commercial service\n• Cache web content (especially rich content: images, video)\n• Deliver from closest edge proxy server\n• Mobile computing\n• devices has limited resources, limited battery power\n• computational offload: offload work to more capable edge server\n• low latency offload important for interactive mobile applications\n• edge server sends results to the mobile\nCompsci 677: Distributed and OS Lec. 20 38\n\nContent Delivery Networks\n• Global network of edge proxies to deliver web content\n• edge clusters of varying sizes deployed in all parts of the world\n• Akamai CDN: 120K servers in 1100 networks, 80 countries\n• Content providers are customers of CDN service\n• Examples: news sites, image-rich online stores, streaming sites\n• Decide what content to cache/offload to CDN\n• Embed links to cdn content: http://cdn.com/company/foo.mp4\n• Consistency responsibility of content providers\n• Users access website normally\n• Some content fetched by browser from CDN cache\nCompsci 677: Distributed and OS Lec. 20 39\nCDN Request Routing\n• Web request need to be directed to nearby CDN server\n• Two level load balancing\n• Global: decide which cluster to use to serve request\n• Local: decide which server in the cluster to use\n• DNS-based approach is common\n• Special DNS server: resolve www.cnn.com/newsvideo.mp4\n• DNS checks location of client and resolves to IP address of nearby CDN server\n• Different users will get resolved to different edge locations\nCompsci 677: Distributed and OS Lec. 20 40\n\nCDN Issues\n• Which proxy answers a client request?\n–Ideally the “closest” proxy\n–Akamai uses a DNS-based approach\n• Propagating notifications\n–Can use multicast or application level multicast to reduce overheads (in push-based\napproaches)\n• Active area of research\n–Numerous research papers available\nCompsci 677: Distributed and OS Lec. 20 41\nCDN Request Processing\n• The principal working of the Akamai CDN.\nCompsci 677: Distributed and OS Lec. 20 42\n\nCDN Hosting of Web Applications\n• Figure 12-21. Alternatives for caching and replication with Web applications.\nCompsci 677: Distributed and OS Lec. 20 43\nMobile Edge Computing\n• Use case: Mobile offload of compute-intensive tasks\n• Example: augmented reality, virtual reality (mobile AR/VR)\n• mobile phone or headset has limited resources, limited battery\n• Low latency / response times for interactive use experience\n• mobile devices may lack a GPU or mobile GPU may be limited\n• Today’s smartphones are highly capable (multiple cores, mobile GPU, neural processor)\n• mobile offload more suitable for less capable devices (e.g., AR headset)\n• 5G cellular providers: deploy edge servers near cell towers\n• industrial automation, autonomous vehicles\nCompsci 677: Distributed and OS Lec. 20 44\n\nEdge Computing Today\n• Emerging approach for latency-sensitive applications\n• Edge AI: run AI (deep learning) inference at edge\n• home security camera sends feed, run object detection\n• Low latency offload: autonomous vehicles, smart city sensors, smart home etc.\n• Edge computing as an extension to cloud\n• Cloud regions augmented by local regions\n• Local regions are edge clusters that offer normal cloud service (but at lower latency) E.g.,\nAWS Boston region\n• Internet of Things (IoT) data processing sevices\nCompsci 677: Distributed and OS Lec. 20 45\nSpecialized Edge Computing\n• Edge accelerators: special hardware to accelerate edge tasks on resource\nconstrained edge servers\n• Nvidia Jetson GPU, Google edge Tensor processing Unit (TUP), Intel\nVision Processing Unit (VPU)\n• Example: IoT ML inference on edge accelerators\n• Efficient inference on resource-constrained edge servers\nApple Neural\nGoogle Edge TPU\nNvidia Jetson Nano GPU\nEngine\nCompsci 677: Distributed and OS Lec. 20 46\n\nCloud and Edge Architectures\n•\nOffload to cloud, edge, specialized edge,\ncloud cloud server\n+ GPU/FPGA\ncloud\nedge nodes\nEdge node\n+ VPU/TPU\nIoT device\nIoT device\nIoT device with accelerator\nTraditional cloud\nTraditional edge Specialized\n(2-tier)\n(3-tier) (3-tier)\nCompsci 677: Distributed and OS Lec. 20 47"
  },
  "Lec20_notes": {
    "url": "http://lass.cs.umass.edu/~shenoy/courses/677content/notes/spring25/Lec20_notes.pdf",
    "parsed_text": "CMPSCI 677 Distributed Operating Systems Spring 2025\nLecture 20: April 16\nLecturer: Prashant Shenoy Scribe: Lian Fu(Spring 2025),Suvid Sahay(Spring 2024)\nIn this lecture, the professor starts a new topic “Distributed Web Applications”.\n20.1 Traditional Web Based Systems\nThe web is basically a client server based global distributed system. The clients tend to be web browsers\nor application on the phone or other devices, which access the server component over HTTP. HTTP is a\nrequest response protocol, as also seen in Lab 2.\nFigure 20.1 shows a basic client-server request response protocol, that exchanges static web pages. But the\nserver side can consist of a complicated server that can process requests. In this image, the browser sends\nan HTTP request to the web server. The server fetches the document from database and sends back the\nresponse to the browser.\nFigure 20.1: Overall Organization of a Traditional Web Site\n20.2 Web Browser Client\nBrowsers are complex with many built-in functionalities. Web browsers have a user interface where the user\ncan submit a request, the browser connects to the server, fetches a web page and renders that web page. As\nin the figure 20.2, there are multiple components such as user interface, browser engine (part that retrieves\nthe content), rendering engine (take the content, decides the layout, etc.) and other components such as a\nnetwork component for network communication, client side scripts for interpretation (example: Javascript\ninterpretation), and a parser that can parse HTML/XML.\n20-1\n\n20-2 Lecture 20: April 16\nFigure 20.2: Logical Components of a Web browser\nFigure 20.3: Overall Organization of Apache Web Server\n20.3 Apache Web Server\nThe server also has complex components. Figure 20.3 is a Apache Web Server, one of the most popular\nweb servers. The original version is a multiprocess model, other variants also support multithreading. By\ndefault, it uses the multiprocess model when starting up. The main process listens on HTTP and assigns\nincoming requests to child processes. Similarly, the multithreaded model can also be implemented with\ndynamic thread pools.\nThe architecture shows the process that takes place, irrespective of whether it is multithreaded or multipro-\ncess. It has a modular architecture that uses pipeline processing. Processing is done by the various modules\npresent (HTTP, SSL, etc.). These can be plugged in when the server is started. We need to configure a\nset of modules that will perform request processing. When a request comes in, it will go through a series\nof processing steps. Each module performs partial processing in a pipeline fashion. ’Hooks’ are used to call\nthese modules. You can turn each module on or off. The only processing that is supported by the server is\nif we write the application using PHP. Other languages such as Java, Python will be processed outside the\nHTTP server.\n\nLecture 20: April 16 20-3\n20.4 Proxy Server\nProxy is an intermediary between the client and the server, making the client-server architecture a client-\nproxy-server architecture. Proxies can be used for multiple purposes. The example shown in figure 20.4\ndoesprotocoltranslation, convertingfromHTTPtoFTP.Proxiesarealsousedforwebcachingandcontent\ntranslation (example: While trying to watch a video on a phone, a server might have the video with a\nresolution higher than is supported by the phone screen. A proxy can translate the high-resolution video to\na lower resolution supported by the phone). A proxy is closer to the client and can process user requests\nfaster than the actual server. Recently used web pages are cached. The browser will send requests to the\nproxyinsteadofserver. Proxywillprocesstherequest. Iftherequestedcontentiscachedonproxy, itiswill\nsend back reply to client. Else, the proxy makes another request to the server.\nFigure 20.4: Web Proxy\n20.5 Mutlitiered Architecture\nFigure 20.5: Multitiered architecture\nThe figure 20.5 represents a standard 3-tier web architecture wherein requests are sent from the web server,\nprocessed by the CGI program (which is now replaced by Python, Java, etc.) and then to the database.\nThe request is processed and the response is sent back to the client. In this case as well, a proxy can be\nintroduced to avoid repetitive computation.\n\n20-4 Lecture 20: April 16\nFigure 20.6: Web Server Clusters\n20.6 Web Server Clusters\nFigure 20.6 shows an example of clustered architecture. Each box in itself is multi-tiered. Each incoming\nrequest can be forwarded to one of the four Web Servers. The front end handles all the incoming requests\nand the outgoing responses. Responses flow back through the load balancing switch. This is a common way\nto scale applications. If a single machine is not enough, we replicate the web application. The clusters can\nservice more requests, which allows it to scale up better.\nThere are 2 ways in which this can be implemented:\n1. The load balancer receives all the HTTP requests and then forwards it to the app tier, whose multiple\nreplicas are present. Each app tier replica has its own database replica. If the database is updated\nfrequently, it also needs to be synchronized frequently. Thus, this is mostly used in databases that are\nread heavy and are infrequently updated. This supports distributed replicas.\n2. The load balancer receives all the HTTP requests and then forwards it to the app tier, but all the\nmultiple app replicas have a common database tier, that is they connect to the same component. This\nis more commonly used by multiple applications. In this case, no data synchronization is required.\nOnly the web tier and app tier are replicated. This works well as long as bottleneck is not I/O. In\nmost cases, the bottleneck is request processing, thus this is not a problem. However, this way does\nnot support having distributed replicas of the database, unlike method 1 above.\nThere are different ways in which the request can be forwarded to the servers in fig 20.6:\n1. Request-basedscheduling: EveryHTTPrequestcomingincanbepotentiallysenttoanyreplica. This\ncan be done in cases where there is no state to be saved. To facilitate the storing of a state in request-\nbased scheduling, we can have a common storage where all the replicas can access the state of a client\nfrom.\n2. Session-based scheduling: A browser first establishes a session with the web-server. Once this is done\nand maps to a replica, all requests of that server are sent to the same replica. This is beneficial as\nopposedtorequest-basedschedulingasithelpsincaching,helpsinkeepingthestateinasinglemachine\n(example: saving the state of a shopping cart).\nIrrespective of which way a request is forwarded, the way the load balancer can forward requests to one of\nthe replicas, is either by using HTTP redirect, TCP splicing or TCP handoff. In the context of figure 20.7,\n\nLecture 20: April 16 20-5\nthe switch (load balancer) sends the request to a distributor. The distributor’s goal is to do load balancing.\nThe distributor may also communicate with the dispatcher whether to handle the request on the current\nserver or if it needs to send the request to some other server in order to process it.\n1. HTTP redirect: The load balancer simply redirects the HTTP request to the web server, which then\nprocesses the request and sends the response back.\n2. TCP Splicing: The client sends the HTTP request to the switch, which then makes another request\nto the web server. When the response comes back, the load balancer appends this to the first request\nand sends it back. These are essentially 2 TCP connections that are spliced together.\n3. TCPHandoff: SimilartoHTTPredirectwherewegetaTCPconnectionandwehanditofftoanother\nmachine. It requires network-level changes to handoff using socket connections.\nFigure 20.7: Scalable Content-Aware Cluster of Web servers\nQuestion: In TCP splicing, will the front-end keep the connection alive or construct/deconstruct?\nAnswer: HTTP redirect, TCP Splicing and TCP handoff are independent of whether connections are per-\nsistent or not. In either of the two cases, all these techniques can be applied.\nQuestion: Does TCP splicing have an advantage over TCP handoff?\nAnswer: In TCP handoff, the request is completely handed off to another server whereas in TCP splicing\nthere is a middle entity that works as a relay. This relay can become a bottleneck. In TCP splicing the\nreplicas are hidden.\nQuestion: Can the dispatcher be replicated to remove bottleneck?\nAnswer: To some degree we could. However, the dispatcher might have a state in it. The state also has to\nbe sent to replica for consistency reasons.\nQuestion: What is the actual path of request in an HTTP redirect?\nAnswer: InHTTPredirect,theclientfirstmakesanHTTPconnectiontothefrontend. Oncethefrontend\ndoes an HTTP redirect, the request goes to one of the servers. At that point, the client is directly talking\nto the server.\nQuestion: Can you give some use-cases for these?\nAnswer: They all do the same thing. For HTTP redirect to work, all the machines would have to have\n\n20-6 Lecture 20: April 16\na public IP address so that the client can connect to it. But if we have private IP addresses and only the\nswitch has a public IP, we would not be able to use HTTP redirect, and would instead have to use splicing\nor TCP handoff.\nQuestion: Will HTTP redirect have session information?\nAnswer: When any client sends a request to the switch, the switch keeps a table saying that it received a\nrequestfromthatclient. Itchecksifthisisapartofanexistingsession. Ifso,ittakesthatmachineanddoes\nan HTTP redirect to it or sends the request using any of the other mechanisms. Thus, session/state and\nredirection mechanisms are somewhat orthogonal. However, to an extent, there are schedulers that would\ndo an HTTP redirect and the client can send subsequent requests directly to that server.\nQuestion: Suppose we receive an HTTP request and do some partial processing, and figure out that it\nneeds to be sent to a subsequent process? Can we do the handoff then?\nAnswer: The multitier architecture does exactly that. We don’t have to handoff the request, we can make\nanother request and get its response. How to ask a client to forward all its subsequent requests to another\nserver? This has other mechanisms, but even if some partial processing is done, we can still do an HTTP\nredirect request, as we haven’t sent back a response yet.\nQuestion: Why is the dispatcher sitting separately from the switch?\nAnswer: This is an older architecture where the switch directly forwards the request to the web server and\nthe server’s distributor talks to dispatcher to determine whether to handle request on the same machine or\nforward the request to another machine. In more recent architectures, the dispatcher sits along with the\nswitch to directly forward the request to the right web server.\nQuestion(Instructor): Why we use the response directly goes back from the web server to the client as\nopposedtothesimplerapproachwherealltheinteractionsorthebrowserareactuallygoingthroughasingle\npoint which is the front end node of the application?\nAnswer: Sendingresponsesdirectlyfromthereplicabypassesthefront-endforthereturnpath,reducingits\nworkload (specifically handling outgoing response traffic) and making it less likely to become a performance\nbottleneck for the entire system.\nQuestion: Can persistent connections avoid the complexity of direct response mechanisms?\nAnswer: Yes, we can set up a persistent connection that will avoid all of it.\n20.7 Elastic Scaling\nIt is an interesting technique that can be implemented when we have a clustered web application. Web\nworkloads are time-varying (time-of-day effects, seasons when the workload is high, etc). There are other\nkinds such as load spikes or flash crowds, wherein the workloads increase suddenly (example: news story\nbreaks). Somemaybeexpected,suchassportsevents,bigsales,andsoon. Howtodealwiththesechanging\nworkloads?\nOne approach is to decide the absolute maximum workload that the service will see and put enough servers\nto be able to handle it. But many-a-times, a lot of the servers would be sitting idle. Also, it is not always\npossibletopredictthetraffic. Asaresult,applicationsaregenerallyunder-provisioned,whereintheworkload\nexceeds the capacity even when there are multiple replicas.\nElastic scaling/auto-scaling: Increase the capacity on the fly with increasing or decreasing loads. The web\n\nLecture 20: April 16 20-7\nserver monitors its threshold and adds servers when this threshold is being approached. This can be done\nprogrammatically in cloud applications. There are 2 ways:\n1. Horizontal scaling: There are multiple replicas and we add or remove those based on the load.\n2. Vertical scaling: We don’t change the number of the clusters, but change the size of the replicas, by\ngiving or removing cores.\nThis is used widely in modern cloud-based applications.\nWhen do we scale?\n1. Proactive Scaling: Look ahead and predict workloads (maybe for the next hour) and pre-provision\nresources ahead of time.\n2. Reactive scaling: This is when we don’t do any provisioning but monitor the load. For example, we\nonly add new machines when the load reaches 70% or 80%. This may lead to small disruptions due to\nthe time taken to start the machines.\nQuestion: Ifthere’sawebapplicationthatisusingaprivatecloudanditsworkloadgoesup,willitoverflow\nin the public cloud?\nAnswer: It depends on the current utilization of the available resources (private/public/hybrid cloud). If\nprivate cloud has available machines there is no need to go to a public cloud. If there are no machines in\nthe private cloud, then the application can be overflown into public loud. In such a case the load balancer\nhas to be intelligent enough to determine which requests to send to the private cloud and which to send to\nthe public cloud. Elastic scaling techniques apply in all of the above scenarios.\nQuestion: Does elastic scaling deal with workload increase due to malicious requests or web crawlers?\nAnswer: Elastic Scaling doesn’t look at the cause of the workload increase. It will simply increase the\ncapacity in the case of increased workload.\nQuestion: In vertical scaling, can we increase the number of resources without restarting?\nAnswer: It can be done with specific implementation of VMs. for eg. a VM can have 10 virtual cores\nmapped to 2 physical cores and the number of cores can be later increased to 3 without having to restart\nthe VM.\nQuestion: In practice which one is used? Horizontal or vertical scaling?\nAnswer: Most large services use a combination of the above techniques to ensure that they never run out\nof capacity. But mostly horizontal scaling is used because it’s easier to just start an instance rather than\nhaving a special VM for increasing/decreasing the resources allocated.\nQuestion: Where do the new servers added during elastic scaling come from (idle pool vs. borrowed from\nother users)?\nAnswer: In public cloud environments (AWS, Azure, GCP), scaling typically utilizes readily available idle\nservers from the provider’s large pool (over-provisioned capacity). Servers are not taken away from other\ncustomers. However, within a single large enterprise with its own infrastructure (like Google), capacity\nmight be dynamically shifted (preempted/borrowed) from lower-priority tasks (e.g., batch jobs) to meet the\ndemands of higher-priority, user-facing services during load spikes.\n\n20-8 Lecture 20: April 16\nQuestion: Can vertical scaling (increasing a server’s resources) be done without restarting the server?\nAnswer: Yes, often it can be done live. Common methods include:\n1. Dynamic VM Resource Allocation: Cloud platforms can often allocate more CPU cores or memory to a\nrunningVirtualMachine(VM)iftheunderlyingphysicalhosthasavailablecapacity, withoutneedingaVM\nrestart.\n2. LiveMigration: TheentirerunningVMcanbeseamlesslymigratedtoadifferent,morepowerfulphysical\nhost with minimal or no perceived downtime for the application.\n20.8 Microservices Architecture\nFigure 20.8: Scalability cube which shows 3 ways of scaling\nEachapplicationisacollectionofsmallerservices. Wetakeanapplicationtier,whichisbasicallyamonolithic\napplicationtier,andsplititintosmallercomponents,eachofwhichisamicroservice. Thisisalsoanexample\nofaservice-orientedarchitecture. Thisgivesmodularityandwecanchangeeachmicroserviceindependently,\nwithout affecting the other microservices and making it easier to manage. Teams can be responsible for one\nservice. These can be independently deployed as well. Each microservice can be clustered and auto-scaled.\nExample: wecanscaleuponlythatmicroservicewhichiscomputeintensive. Butthismakestheapplication\nlook more complicated. This is one more way of scaling web applications. The figure 20.8 shows 3 ways of\nscaling. The x-axis is horizontal scaling. z-axis is called data partitioning: we partition the data instead\nof replicating it. If we don’t want to replicate our data but it becomes a bottleneck, one way is to split\nthe data into parts and put it onto different machines. This is sharding or partitioning. The y-axis shows\nfunctional decompostion, wherein we take different microservices and scale them independently. We can use\nany combination of these, typically all the 3 are used.\nQuestion: Why is data partitioning a way of scaling?\nAnswer: Let’s say we have an application that has 100,000 users. You can write it in a way where each\nreplica is capable of serving all 100,000 users, or you can write it in a way where each replica handles a\nfraction of users. That’s data partitioning. Each replica is responsible for some subset of users and their\ndata. A request is sent to the right replica that holds the user’s data. As users grow, we can repartition\nthese responsibilities.\nQuestion: In data partitioning, are data and servers both partitioned?\nAnswer: In data partitioning there are multiple servers. Rather than saying that any server can serve any\nuser or access any data, the responsibility is partitioned to specific servers. One server is responsible for a\nsubset of data, another server takes care of a different subset of data, and so on.\n\nLecture 20: April 16 20-9\nQuestion: How does data partitioning handle server crashes?\nAnswer: If data is partitioned without redundancy, a server crash leads to data loss or unavailability for\nthat partition. To mitigate this:\n1. Replication: Each data partition must be replicated onto one or more other servers. This requires\nmechanisms to keep the replicas consistent (e.g., primary-backup synchronization, Paxos/Raft for stronger\nconsistency).\n2. Backup & Restore: Rely on taking periodic backups of partitions to external storage. If a server crashes,\nits partitions can be restored from the backup onto a new server, but this involves downtime during the\nrestore process and potential loss of data generated since the last backup.\n20.9 Web Documents\nMost web browsers get back content, mostly an HTML page with embedded content or objects. The figure\n20.9 shows the types of objects. The simplest is the text object. The content is encoded using MIME. Web\nbrowsers extract it, parse it and render.\nFigure 20.9: Web Documents\n20.10 HTTP Connections\nFigure20.10showstheoriginalHTTP1.0, wherebrowsersetsupanewTCPconnectiontotheserverevery\ntime we make a HTTP request. Each entity or object we get back has its own connection. Once we get\na response, we tear down the connection. That is, it is non-persistent. Making a new connection to the\nsame server every time is wasteful. A variant, version 1.1 20.11 is more efficient as it creates persistent\nconnections. We send the next HTTP requests over the same connection. The browser does not close the\nconnection immediately, in anticipation of further requests. However, these are sequential. We need to get\nthe response to the previous request before sending the new request. This can be slow and many browsers\ndon’t use this mechanism. For multiple simultaneous downloads, we set up multiple connections instead.\nThus, HTTP1.1 did not solve its goal of saving connection overhead due to sequential requests.\n\n20-10 Lecture 20: April 16\nFigure 20.10: HTTP 1.0: Using Non-Persistent Connections.\nFigure 20.11: HTTP 1.1: Using Persistent Connections.\n20.10.1 HTTP Methods\nHTTP protocol has five methods:\n1. The simplest among them is the ’GET’ command. It takes a URL and simply fetches what the URL\nis pointing to, which can be an HTML page, an image, etc.. It is the most used HTTP command.\n2. While ’PUT’ command is used to store a document on the server.\n3. ’POST’allowsustoadddatatothedocument. Wheneverwesubmitwebforms,itisa’POST’request.\n4. ’DELETE’ can be used to delete a document, but most web browsers do not support it.\n5. ’HEAD’ gets the header of the document. It is typically used for caching.\nQuestion: Is ’PUT’ a holdover from FTP or some prior protocol?\nAnswer: ’PUT’ was designed from FTP’s perspective, but it is still used for other purposes, although not\nas much as ’GET’ and ’POST’.\nQuestion: What is the difference between ’PUT’ and ’POST’?\nAnswer: ’PUT’isusedtosendentirefilestotheserver. ’POST’isgenerallyusedtosendapartofthedata\nbut not the entire data. Web forms are a good example to think of when talking about the ’POST’ HTTP\nmethod. The fields of the form are sent as individual fields (name, email, etc.) and not as a single entity.\n\nLecture 20: April 16 20-11\n20.11 HTTP 2.0\nHTTP 2.0 is designed to address the message latency problem. It allows us to have binary headers. We can\ncompressheadersandmessages,makingthemessagesmallerandthusfaster. InHTTP2.0responsesarenot\nguaranteed to be in sequential order. It allows concurrent connections (persistent but with concurrency),\nthuswedonothavetowaitforresponsesforthepreviousrequests. Thisisdoneusingtheconceptofstreams.\nEach stream is one request and one response. To send a request, we send it using a new stream, thus when\nthe response comes, we know which request it is intended for. This helps in speeding up the connection\nsignificantly. Both the browser and the server have to support HTTP 2.0. It is not backward compatible.\nHTTP2.0hasbidirectionalfunctionalitywheretheservercanpushdatatotheclient. ThisisbecauseHTTP\n2.0 is a push based protocol(as compared to HTTP 1.0 which is a pull based protocol). for eg. If the server\nhas active connections to the client, the server can push updates rather than the client polling the server.\nQuestion: Is there a new version that is going to use UDP?\nAnswer: Yes, there is. HTTP 1.0 and 1.1 only used TCP. HTTP 2.0 can run on different protocols, in-\ncluding UDP. Google version of UDP is QUIC which is used when the chrome browsers connect to google\nservers. But here all features of TCP such as 3 way handshake are not available and have to be taken care\nof at the application level.\nQuestion: Howistheserverdesigned? Willtherebemultiplethreadsforoneconnectionoreachconnection\nwill have a single thread ?\nAnswer: This is an implementation detail. HTTP 2.0 does not define this. An application developer has\nto take care of this.\nQuestion: In HTTP 2.0, are multiple connections preferred over a single connection?\nAnswer: In HTTP 2.0 the idea is to use one connection to send multiple requests in parallel and get re-\nsponses in some order. We try not to have multiple connections. We can still use multiple connections, but\nit is designed to use a single connection.\nQuestion: Is QUIC intended for the global internet or just smaller networks?\nAnswer: QUIC was designed and is used for communication over the wide-area internet (worldwide web).\nItoriginatedfromGoogle’seffortstoimproveprotocolsforserviceslikestreaming(YouTube)overthepublic\ninternet, running on top of UDP. It’s not restricted to smaller, controlled environments like data centers.\nEffective use requires support from both the client (e.g., browser) and the server.\n20.12 Web Services Fundamentals\nWebservicesarewaysinwhichwecanwriteapplicationsanduseRPCsbetweentheseapplicationsorbetween\naclientandaserver. Thetermwebservicehasaspecificconnotationwhereweuseacertaintypeofinterface\ndescription language, a certain protocol for SOAP for us to send RPCs. We use an interface definition\nlanguage called webservice definition language (WSDL), a compiler that generates stubs for the client and\nthe server, the protocol SOAP (like HTML but is XML based) used for communication. This can be seen\nin fig 20.12.\nSOAP,SimpleObjectAccessProtocol,wasusedtomakeRPCrequestsoverHTTP.Fig20.13isanexample\nofmakinganRPCrequestoverSOAP.TheentireRPCrequestissentasaXMLdocument. Theserverafter\nreceiving the document, parses it, perform required operation and sends back the response as another XML\n\n20-12 Lecture 20: April 16\nFigure 20.12: Web service\ndocument. Figure 20.13 shows one such XML request document. Here the client is calling ’alert’ method\nand passing the string ’Pick up Mary at school at 2pm’ as an argument to that method.\nFigure 20.13: Example of XML-based SOAP message\n20.13 Restful Web Services\nAs we can see, for calling single method with one argument, we have to send such a long XML file. SOAP\ndid not perform well because of this overhead. As a result, SOAP evolved into Restful architecture. Rather\nthanusingXMLtosenddata,itusesHTTPtosendrequestandgetaresponse. HTTPwasalreadypopular\nthansomethinglikeSOAP,soitwaschosenasawaytomakeRPCrequests. IncaseofRestfularchitectures,\nthe communication is light weight and assumes one-to-one communication between client and server.\nIn Resfult webservices, we use:\n\nLecture 20: April 16 20-13\n1. GET: to read something\n2. POST: to create, update or delete something\n3. PUT: to create or update something\n4. DELETE: to delete something\nThe example in 20.14, a GET request is made and the response is received. The response sends an XML\npacket in this case, but could also be JSON, any format can be used. It is much more compact than using\nSOAP.\nFigure 20.14: Example of a Restful web service\n20.14 SOAP VS RESTful WS\n1. SOAP application can be written in various languages and can run on different OS or platforms and\nis also transport agnostic (XML, TCP, etc. cab be used). It need not use HTTP. Whereas, RESTful\nservices only support HTTP. Restful services, however, can use any language to write this, such as\nPython, Java, and so on.\n2. SOAP is for general-purpose distributed systems. We can have all kinds of applications making calls\ntoeachother. Restfulwebserviceshaveaclientandaserverandarepoint-to-point. Wehavepairwise\ninteractions, just like traditional RPC.\n3. SOAP has a wide set of standards (telling us how to write, compile, etc/), whereas RESTful services\ndo not have any pre-defined standards. They have general guidelines, but no one particular way to\nwrite them.\n4. SOAP is very heavyweight compared to REST.\n5. Rest has less of a learning curve compared to SOAP."
  }
}